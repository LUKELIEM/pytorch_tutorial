{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn PyTorch\n",
    "\n",
    "Step-by-step mastering of PyTorch:\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.3\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.0030e+17  4.5642e-41 -1.0030e+17\n",
      " 4.5642e-41  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00\n",
      " 8.9683e-44  0.0000e+00  1.5695e-43\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.3232  0.6961  0.7682\n",
      " 0.0437  0.1695  0.4603\n",
      " 0.0755  0.0513  0.5006\n",
      " 0.7241  0.9199  0.4430\n",
      " 0.6940  0.1012  0.7336\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "torch.Size([5, 3])\n",
      "\n",
      " 0.0650  0.4111  0.9006\n",
      " 0.3754  0.6471  0.6847\n",
      " 0.3545  0.0664  0.0459\n",
      " 0.4200  0.3889  0.0885\n",
      " 0.3900  0.2364  0.9111\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.3882  1.1072  1.6688\n",
      " 0.4191  0.8166  1.1450\n",
      " 0.4300  0.1176  0.5465\n",
      " 1.1441  1.3089  0.5315\n",
      " 1.0840  0.3375  1.6447\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.3882  1.1072  1.6688\n",
      " 0.4191  0.8166  1.1450\n",
      " 0.4300  0.1176  0.5465\n",
      " 1.1441  1.3089  0.5315\n",
      " 1.0840  0.3375  1.6447\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.3882  1.1072  1.6688\n",
      " 0.4191  0.8166  1.1450\n",
      " 0.4300  0.1176  0.5465\n",
      " 1.1441  1.3089  0.5315\n",
      " 1.0840  0.3375  1.6447\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3)   # uninitialized\n",
    "print(x)\n",
    "\n",
    "x = torch.rand(5, 3)  # randomized\n",
    "print(x)\n",
    "\n",
    "print(x.size())  # torch.Size is a tuple\n",
    "\n",
    "y = torch.rand(5, 3)\n",
    "print(y)\n",
    "\n",
    "# Three ways to add Tensors\n",
    "\n",
    "print(x + y)  # Way 1\n",
    "\n",
    "print(torch.add(x, y))  # Way 2\n",
    "\n",
    "result = torch.Tensor(5, 3)  # Way 3\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In place operation\n",
    "\n",
    "Any operation that mutates a tensor in-place is post-fixed with an _. For example: x.copy_(y), x.t_(), will change x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0650  0.4111  0.9006\n",
      " 0.3754  0.6471  0.6847\n",
      " 0.3545  0.0664  0.0459\n",
      " 0.4200  0.3889  0.0885\n",
      " 0.3900  0.2364  0.9111\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.3882  1.1072  1.6688\n",
      " 0.4191  0.8166  1.1450\n",
      " 0.4300  0.1176  0.5465\n",
      " 1.1441  1.3089  0.5315\n",
      " 1.0840  0.3375  1.6447\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing, Reshape, etc.\n",
    "\n",
    "Indexing and slicing is similar to Numpy.\n",
    "\n",
    "Resizing: If you want to resize/reshape tensor, you can use torch.view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.0060  0.7897  0.1215  1.4496\n",
      "-0.6373 -0.0289 -0.0835 -0.2037\n",
      "-0.8296 -1.1209  2.0529  1.3891\n",
      " 1.1334 -0.1597  0.0246  2.0013\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "\n",
      " 0.7897\n",
      "-0.0289\n",
      "-1.1209\n",
      "-0.1597\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "\n",
      "-1.2132\n",
      "-1.2730\n",
      "-1.0381\n",
      "-0.8865\n",
      " 0.0797\n",
      "-0.2809\n",
      "-0.8449\n",
      " 0.4542\n",
      "-1.4533\n",
      "-0.7981\n",
      "-1.0626\n",
      " 0.8023\n",
      " 1.1387\n",
      " 0.7386\n",
      " 0.8643\n",
      " 0.9609\n",
      "[torch.FloatTensor of size 16]\n",
      "\n",
      "\n",
      "-1.2132 -1.2730 -1.0381 -0.8865  0.0797 -0.2809 -0.8449  0.4542\n",
      "-1.4533 -0.7981 -1.0626  0.8023  1.1387  0.7386  0.8643  0.9609\n",
      "[torch.FloatTensor of size 2x8]\n",
      "\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[:, 1])  # indexing, slicing\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "print(y)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(z)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy Bridge\n",
    "\n",
    "The Torch Tensor and NumPy array will share their underlying memory locations, and changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "torch.Size([5])\n",
      "(5,)\n",
      "[ 2.00629776 -0.11403814 -3.50661283]\n",
      "(5, 1)\n",
      "[[ 2.00629776]\n",
      " [-0.11403814]\n",
      " [-3.50661283]]\n",
      "[ 3.  3.  3.  3.  3.]\n",
      "[[ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [ 3.]]\n"
     ]
    }
   ],
   "source": [
    "# Converting a Torch Tensor to a NumPy Array\n",
    "\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "print(a.size())\n",
    "\n",
    "W = np.random.randn(3,5)\n",
    "\n",
    "b = a.numpy()   # \n",
    "print(b.shape)  \n",
    "print(np.dot(W,b))\n",
    "\n",
    "c = a.numpy().reshape(-1,1)  \n",
    "print(c.shape)\n",
    "print(np.dot(W,c))\n",
    "\n",
    "a.add_(2)   # Note that changing the Tensor a will change the numpy arrays b and c\n",
    "print (b)\n",
    "print (c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting NumPy Array to Torch Tensor\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)\n",
    "np.add(a, 1, out=a)  # changing numpy a will change tensor b\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda\n",
    "\n",
    "Tensors can be moved onto GPU using the .cuda method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y\n",
    "    \n",
    "print (torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: automatic differentiation\n",
    "\n",
    "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "http://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "\n",
    "autograd.Variable is the central class of the package. It wraps a Tensor, and supports nearly all of operations defined on it. Once you finish your computation you can call .backward() and have all the gradients computed automatically.\n",
    "\n",
    "You can access the raw tensor through the .data attribute, while the gradient w.r.t. this variable is accumulated into .grad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "<AddBackward0 object at 0x7f3bd598ae10>\n",
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn.\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient and Function\n",
    "\n",
    "There’s one more class which is very important for autograd implementation - a Function.\n",
    "\n",
    "Variable and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each variable has a .grad_fn attribute that references a Function that has created the Variable (except for Variables created by the user - their grad_fn is None).\n",
    "\n",
    "If you want to compute the derivatives, you can call .backward() on a Variable. If Variable is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a grad_output argument that is a tensor of matching shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MeanBackward1 object at 0x7f3bd598a0b8>\n",
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# y was created as a result of an operation, so it has a grad_fn.\n",
    "print(out.grad_fn)  # out a Variable created by z.mean()\n",
    "\n",
    "out.backward()  # out.backward() is equivalent to doing out.backward(torch.Tensor([1.0]))\n",
    "\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What happened up there??? **\n",
    "\n",
    "We have that \n",
    "$$ o = {1\\over4} {∑ z_{i}} $$\n",
    "$$ z_{i} = 3{(x_{i}+2)}^2 $$\n",
    "\n",
    "Therefore, \n",
    "$$ {∂o \\over ∂x_{i}} = {{3 \\over 2} {(x_{i}+2)}}$$\n",
    "\n",
    "When $ x_{i} = 1 $, $ {∂o \\over ∂x_{i}} = 4.5 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.9765\n",
      " 1.4325\n",
      " 1.6437\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "2.9428786269703626\n",
      "5.885757253940725\n",
      "11.77151450788145\n",
      "23.5430290157629\n",
      "47.0860580315258\n",
      "94.1721160630516\n",
      "188.3442321261032\n",
      "376.6884642522064\n",
      "753.3769285044128\n",
      "Variable containing:\n",
      " 1011.9902\n",
      "  733.4418\n",
      "  841.5737\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      "  102.4000\n",
      " 1024.0000\n",
      "    0.1024\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "print (y)\n",
    "\n",
    "while y.data.norm() < 1000:\n",
    "    print (y.data.norm())\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What happened up there? **\n",
    "\n",
    "\n",
    "We have that \n",
    "$$ y = 2x $$\n",
    "$$ y = y * 2^9 = x * 2^{10} $$\n",
    "\n",
    "Therefore, \n",
    "$$ {∂y \\over ∂x} = 1024$$\n",
    "\n",
    "When upstream gradient is [0.1, 1.0, 0.0001]\n",
    "\n",
    "$$ {∂f \\over ∂x} = {∂f \\over ∂y}{∂y \\over ∂x} = \\begin{pmatrix}\n",
    "0.1\\\\\n",
    "1.0\\\\\n",
    "0.0001\n",
    "\\end{pmatrix} 1024 = \\begin{pmatrix}\n",
    "102.4\\\\\n",
    "1024.0\\\\\n",
    "0.1024\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The ability to inject gradient into a computation graph is important for policy gradient implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
