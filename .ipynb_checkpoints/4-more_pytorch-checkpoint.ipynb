{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More about PyTorch\n",
    "\n",
    "A lot of implementation details are missing in the PyTorch Tutorial. We will work on a couple of things that will be important later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Python version:  3.6.3\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization at Tensor Level\n",
    "\n",
    "At the tensor level, we can use nn.init to indirectly initialize the content of the Tensor.\n",
    "\n",
    "There is another more direct way using .data.normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.9550 -0.4278 -0.2605  0.5902  0.6849\n",
      "-0.0509 -0.6427 -2.3600  0.8400 -0.7931\n",
      " 0.6833 -0.3046  0.3007 -0.5518 -0.4444\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n",
      "<class 'torch.FloatTensor'>\n",
      "Variable containing:\n",
      " 0.6825 -0.0320  0.0557  1.1974 -0.2419\n",
      " 0.7679 -0.3447 -0.4307 -1.4092 -0.5008\n",
      " 1.0139  1.9033  2.7284  0.1253  0.3340\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "w = torch.Tensor(3, 5)\n",
    "torch.nn.init.normal(w)  # This is like np.random.randn()\n",
    "\n",
    "print (w)\n",
    "print (type(w))\n",
    "\n",
    "# work for Variables also\n",
    "w2 = Variable(w)\n",
    "torch.nn.init.normal(w2)\n",
    "\n",
    "print (w2)\n",
    "print (type(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.1246 -1.0840 -0.0515 -0.2475  0.3643\n",
      "-0.5712  1.1921 -0.8186  0.5920 -0.5610\n",
      " 0.3099 -0.5385 -1.3637 -0.2489 -2.5477\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# old styled direct access to tensors data attribute\n",
    "w2.data.normal_()  # inplace modify\n",
    "print (w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization at Module Level\n",
    "\n",
    "At the module level, we can use .apply(fn) to initialize the weights in the module. Note that we need to write the fn function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the functions that will be used to operate on the submodules used by .apply(fn)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        print(\"Apply weight init on \",classname)\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        print(\"Apply weight init on \",classname)\n",
    "        m.weight.data.normal_(0.0, 0.01)\n",
    "        m.bias.data.fill_(0)    \n",
    "        \n",
    "def display_weights(m):\n",
    "\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        print(classname)\n",
    "        print(m.weight.data)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        print(classname)\n",
    "        print(m.weight.data)\n",
    "        print(m.bias.data)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        print(classname)\n",
    "        print(m.weight.data)\n",
    "        print(m.bias.data)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "        \n",
    "simple_model = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=7, stride=2),  # 3 input image channel, 32 output channels\n",
    "                nn.ReLU(inplace=True),\n",
    "                Flatten(), # see above for explanation\n",
    "                nn.Linear(5408, 10), # affine layer\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .apply(fn)\n",
    "\n",
    "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init)\n",
    "\n",
    "http://pytorch.org/docs/master/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply weight init on  Conv2d\n",
      "Apply weight init on  Linear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d (3, 32, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Flatten(\n",
       "  )\n",
       "  (3): Linear(in_features=5408, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.apply(weights_init)\n",
    "\n",
    "# simple_model.apply(display_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply weight init on  Conv2d\n",
      "Apply weight init on  Conv2d\n",
      "Apply weight init on  Linear\n",
      "Apply weight init on  Linear\n",
      "Apply weight init on  Linear\n"
     ]
    }
   ],
   "source": [
    "# The neural net is defined as a class\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        # Added as a standard direct initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                classname = m.__class__.__name__\n",
    "                print(\"Apply weight init on \",classname)\n",
    "                m.weight.data.normal_(0.0, 0.01)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                classname = m.__class__.__name__\n",
    "                print(\"Apply weight init on \",classname)\n",
    "                m.weight.data.normal_(0.0, 0.01)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
