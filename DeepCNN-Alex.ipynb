{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE253 PA3 - Design a CNN \n",
    "\n",
    "First a bit of setup!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.5.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import timeit\n",
    "import time\n",
    "import platform\n",
    "import random\n",
    "import pickle as pickle\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "We load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class ChunkSampler(sampler.Sampler):\n",
    "    \"\"\"Samples elements sequentially from some offset. \n",
    "    Arguments:\n",
    "        num_samples: # of desired datapoints\n",
    "        start: offset where we should start selecting from\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples, start = 0):\n",
    "        self.num_samples = num_samples\n",
    "        self.start = start\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.start + self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "NUM_TRAIN = 40000\n",
    "NUM_VAL = 10000\n",
    "\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_train = (DataLoader(cifar10_train, batch_size=64, sampler=ChunkSampler(NUM_TRAIN, 0)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                           transform=T.ToTensor())\n",
    "loader_val = (DataLoader(cifar10_val, batch_size=64, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True,\n",
    "                          transform=T.ToTensor())\n",
    "loader_test = (DataLoader(cifar10_test, batch_size=64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our image data (and more relevantly, our intermediate feature maps) are initially N x C x H x W, where:\n",
    "\n",
    "* N is the number of datapoints  \n",
    "* C is the number of channels  \n",
    "* H is the height of the intermediate feature map in pixels  \n",
    "* W is the height of the intermediate feature map in pixels  \n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that needs spatial understanding of where the intermediate features are relative to each other. When we input data into fully connected affine layers, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"Flatten\" operation to collapse the C x H x W values per representation into a single long vector. The Flatten function below first reads in the N, C, H, and W values from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be C x H x W, but we don't need to specify that explicitly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor # the CPU datatype\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "        \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, num_epochs = 1, verbose=False):\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.9)    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose:\n",
    "            print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "            \n",
    "        scheduler.step()    \n",
    "        model.train()\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(gpu_dtype))\n",
    "            y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0 and verbose:\n",
    "                print('t = %d, loss = %.6f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def train_detailed(model, loss_fn, optimizer, reg, num_epochs = 10, verbose=False):\n",
    "    # Train the model in greater detail - output validation and train accuracy every epoch\n",
    "    \n",
    "    train_history = []   # this will store train accuracy, val accuracy and loss for every epoch\n",
    "    best_val_acc = 0.0  # initialize best_val_acc\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.9)   # decay lr by 0.9 very 5 epochs \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose:\n",
    "            print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "            \n",
    "        scheduler.step()    \n",
    "        model.train()\n",
    "        lossSum = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            x_var = Variable(x.type(gpu_dtype))\n",
    "            y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            lossSum += loss.data[0]\n",
    "            if (t + 1) % print_every == 0 and verbose:\n",
    "                print('t = %d, loss = %.6f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "       \n",
    "        # We calculate validation and training accuracy at the end of every epoch\n",
    "        val_acc = check_accuracy(model, loader_val) \n",
    "        train_acc = check_accuracy(model, loader_train)\n",
    "        print (\"train acc: {} val acc:{}\".format(train_acc,val_acc))\n",
    "        val_loss = check_loss(model,loader_val,verbose)\n",
    "\n",
    "        if val_acc > best_val_acc:  \n",
    "            # save your model \n",
    "            file_name = 'model_reg='+str(reg)+'bestacc.pt'\n",
    "            torch.save(model.state_dict(), file_name)\n",
    "            best_val_acc = val_acc \n",
    "        \n",
    "        print(\"progress is as such:\",[train_acc,val_acc, lossSum/t, val_loss])\n",
    "        train_history.append([train_acc,val_acc, lossSum/t, val_loss])\n",
    "        \n",
    "    return best_val_acc, train_history \n",
    "            \n",
    "def check_accuracy(model, loader, verbose=False):\n",
    "    if verbose:\n",
    "        if loader.dataset.train:\n",
    "            print('Checking accuracy on validation set')\n",
    "        else:\n",
    "            print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for x, y in loader:\n",
    "        x_var = Variable(x.type(gpu_dtype), volatile=True)\n",
    "\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    if verbose:\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc\n",
    "\n",
    "def check_loss(model,loader,verbose):\n",
    "    #calculate validation loss\n",
    "    loss2sum = 0\n",
    "    model.eval()\n",
    "    for t, (x, y) in enumerate(loader_val):\n",
    "        x_var = Variable(x.type(gpu_dtype))\n",
    "        y_var = Variable(y.type(gpu_dtype).long())\n",
    "\n",
    "        scores = model(x_var)\n",
    "            \n",
    "        loss2 = loss_fn(scores, y_var)\n",
    "        loss2sum += loss2.data[0]\n",
    "        if (t + 1) % print_every == 0 and verbose:\n",
    "            print('t = %d, loss = %.6f' % (t + 1,loss2.data[0]))\n",
    "    return loss2sum/t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def generator():\n",
    "    \n",
    "    # Model - 4 layer Conv Layers\n",
    "    \n",
    "    model = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=32),\n",
    "                    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.MaxPool2d(kernel_size=2,stride=2),  # Downsample by half\n",
    "                    nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.MaxPool2d(kernel_size=2,stride=2),  # Downsample by half        \n",
    "                    Flatten(),\n",
    "                    nn.Linear(8192,1024),  # 5408=128*16*16 input size\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(1024,10),\n",
    "                    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def generator():\n",
    "\n",
    "    # Model - 5 layer Conv Layers\n",
    "\n",
    "    model = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=32),\n",
    "                    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.MaxPool2d(kernel_size=2,stride=2),  # Downsample by half\n",
    "                    nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.MaxPool2d(kernel_size=2,stride=2),  # Downsample by half        \n",
    "                    Flatten(),\n",
    "                    nn.Linear(8192,1024),  # 5408=128*16*16 input size\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(1024,10),\n",
    "                    )\n",
    "    return model\n",
    "\n",
    "def generator():\n",
    "\n",
    "    Model - 6 layer Conv Layers\n",
    "\n",
    "    model = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=32),\n",
    "                    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.MaxPool2d(kernel_size=2,stride=2),  # Downsample by half\n",
    "                    nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=256),        \n",
    "                    nn.MaxPool2d(kernel_size=2,stride=2),  # Downsample by half        \n",
    "                    Flatten(),\n",
    "                    nn.Linear(16384,1024),  # 5408=128*16*16 input size\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(1024,10),\n",
    "                    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def generator():\n",
    "\n",
    "    # Model - 5 layer Conv Layers (WIDER)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=32),\n",
    "                    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.MaxPool2d(kernel_size=2,stride=2),  # Downsample by half\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=256, kernel_size=3,stride=1,padding=1), # preserve dimension\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=256),\n",
    "                    nn.MaxPool2d(kernel_size=2,stride=2),  # Downsample by half        \n",
    "                    Flatten(),\n",
    "                    nn.Linear(16384,1024),  # 5408=128*16*16 input siz\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(1024,10),\n",
    "                    )\n",
    "    return model\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def generator():\n",
    "\n",
    "    # Model - 10 layer Conv Layers (WIDER)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1), # preserve dimension,1\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=32),\n",
    "                    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1,padding=1), # preserve dimension,2\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension,3\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension,4\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension,5\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension,6\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension,7\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension,8\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension,9\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension,10\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    Flatten(),\n",
    "                    nn.Linear(131072,1024),  # 5408=128*32*32 input siz\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(1024,10),\n",
    "                    )\n",
    "    return model\n",
    "\"\"\"\n",
    "def generator():\n",
    "\n",
    "    # Model - 10 layer Conv Layers (WIDER)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1,padding=1), # preserve dimension,1\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=32),\n",
    "                    nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,stride=1,padding=1), # preserve dimension,2\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=32),\n",
    "                    nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=2,padding=1), # preserve dimension,3\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1), # preserve dimension,4\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=64),\n",
    "                    nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=2,padding=1), # preserve dimension,5\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # preserve dimension,6\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=128),\n",
    "                    nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=2,padding=1), # preserve dimension,7\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=256),\n",
    "                    nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,stride=1,padding=1), # preserve dimension,8\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=256),\n",
    "                    nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,stride=2,padding=1), # preserve dimension,9\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=512),\n",
    "                    nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # preserve dimension,10\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.BatchNorm2d(num_features=512),\n",
    "                    Flatten(),\n",
    "                    nn.Linear(2048,1024),  # 5408=128*32*32 input siz\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(1024,10),\n",
    "                    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = generator().type(gpu_dtype)\n",
    "\n",
    "x_gpu = torch.randn(64, 3, 32, 32).type(gpu_dtype)\n",
    "x_var_gpu = Variable(x_gpu.type(gpu_dtype)) # Construct a PyTorch Variable out of your input data\n",
    "ans = model(x_var_gpu)        # Feed it through the model! \n",
    "\n",
    "#print (ans.shape)\n",
    "\n",
    "# Check to make sure what comes out of your model\n",
    "# is the right dimensionality... this should be True\n",
    "# if you've done everything correctly\n",
    "np.array_equal(np.array(ans.size()), np.array([64, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy is 0.657. Training time for 1 epoch: 49.54 sec\n"
     ]
    }
   ],
   "source": [
    "model = generator().type(gpu_dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00035, weight_decay=1e-10)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train(model, loss_fn, optimizer, num_epochs=1)\n",
    "val_acc = check_accuracy(model, loader_val)\n",
    "\n",
    "end = time.time()\n",
    "    \n",
    "print('validation accuracy is {0}. Training time for 1 epoch: {1:.2f} sec'.format(val_acc, end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.00011168257865591311, reg = 1e-10, validation accuracy is 0.584  \n",
      "lr = 0.0016268571258444234, reg = 1e-10, validation accuracy is 0.598  \n",
      "lr = 5.9433379071275825e-05, reg = 1e-10, validation accuracy is 0.574  \n",
      "lr = 0.002738903646489327, reg = 1e-10, validation accuracy is 0.593  \n",
      "lr = 0.00023558090023171975, reg = 1e-10, validation accuracy is 0.638  \n",
      "lr = 5.646663925409054e-05, reg = 1e-10, validation accuracy is 0.571  \n",
      "lr = 0.004656427826351652, reg = 1e-10, validation accuracy is 0.512  \n",
      "lr = 0.005440279602084457, reg = 1e-10, validation accuracy is 0.467  \n",
      "lr = 0.0007473177781338236, reg = 1e-10, validation accuracy is 0.644  \n",
      "lr = 0.003355945221102989, reg = 1e-10, validation accuracy is 0.495  \n",
      "lr = 0.00023905473415683688, reg = 1e-10, validation accuracy is 0.653  \n",
      "lr = 2.4186993215858095e-05, reg = 1e-10, validation accuracy is 0.522  \n",
      "lr = 0.0009616903078556671, reg = 1e-10, validation accuracy is 0.656  \n",
      "lr = 0.0034777189162263377, reg = 1e-10, validation accuracy is 0.574  \n",
      "lr = 0.0007230185717747063, reg = 1e-10, validation accuracy is 0.624  \n",
      "lr = 0.002390844797537971, reg = 1e-10, validation accuracy is 0.61  \n"
     ]
    }
   ],
   "source": [
    "stat = []\n",
    "\n",
    "start = time.time()\n",
    "max_count = 10\n",
    "for count in range(max_count):\n",
    "    reg = 1e-10\n",
    "    lr = 10**random.uniform(-5,-2)\n",
    "    \n",
    "    model = generator().type(gpu_dtype)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=reg)\n",
    "    train(model, loss_fn, optimizer, num_epochs=1)\n",
    "    val_acc = check_accuracy(model, loader_val)\n",
    "    \n",
    "    print('lr = {}, reg = {}, validation accuracy is {}  '.format(lr, reg, val_acc))\n",
    "    stat.append([lr, reg, val_acc])\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "print('Training time for 10 epochs: {1:.2f} sec'.format(val_acc, end-start))\n",
    "\n",
    "sorted_stat = sorted(stat, key=lambda x: x[0], reverse=True)\n",
    "print (\"In descending order of learning rate:\")\n",
    "for lr, reg, val_acc in sorted_stat:\n",
    "    print('lr = {}, reg = {}, validation accuracy is {}  '.format(lr, reg, val_acc))    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust Learning Rate \n",
    "\n",
    "We turn down regularization (reg=1e-10), and do a coarse learning rate search over several multi-layer CNN with maxpooling.\n",
    "\n",
    "## 14 layer covnet\n",
    "\n",
    "lr = 0.0014332955883302268, reg = 1e-10, validation accuracy is 0.553  \n",
    "lr = 0.0003488919238298938, reg = 1e-10, validation accuracy is 0.699  \n",
    "lr = 0.0003106013879921905, reg = 1e-10, validation accuracy is 0.695  \n",
    "lr = 0.006529508880491467, reg = 1e-10, validation accuracy is 0.098  \n",
    "lr = 0.0006695355436664427, reg = 1e-10, validation accuracy is 0.667  \n",
    "lr = 0.0030084157499084825, reg = 1e-10, validation accuracy is 0.386  \n",
    "lr = 0.0031498218521473008, reg = 1e-10, validation accuracy is 0.297  \n",
    "lr = 0.0004521058841800273, reg = 1e-10, validation accuracy is 0.687  \n",
    "lr = 0.0008941778789603731, reg = 1e-10, validation accuracy is 0.613  \n",
    "lr = 0.00011263526559213542, reg = 1e-10, validation accuracy is 0.714  \n",
    "lr = 0.004351565396023007, reg = 1e-10, validation accuracy is 0.098  \n",
    "lr = 0.009638819818568978, reg = 1e-10, validation accuracy is 0.098  \n",
    "lr = 0.00046849241080962356, reg = 1e-10, validation accuracy is 0.663  \n",
    "lr = 0.00013463548961918718, reg = 1e-10, validation accuracy is 0.697  \n",
    "lr = 2.4948345072165963e-05, reg = 1e-10, validation accuracy is 0.656  \n",
    "lr = 5.77437580640381e-05, reg = 1e-10, validation accuracy is 0.678  \n",
    "lr = 0.0011608301485078573, reg = 1e-10, validation accuracy is 0.596  \n",
    "lr = 0.0007377113977993159, reg = 1e-10, validation accuracy is 0.625  \n",
    "lr = 0.0003554619545626375, reg = 1e-10, validation accuracy is 0.679  \n",
    "lr = 0.0020421838641083946, reg = 1e-10, validation accuracy is 0.537  \n",
    "Training time for 10 epochs: 1516.63 sec\n",
    "In descending order of learning rate:\n",
    "lr = 0.009638819818568978, reg = 1e-10, validation accuracy is 0.098  \n",
    "lr = 0.006529508880491467, reg = 1e-10, validation accuracy is 0.098  \n",
    "lr = 0.004351565396023007, reg = 1e-10, validation accuracy is 0.098  \n",
    "lr = 0.0031498218521473008, reg = 1e-10, validation accuracy is 0.297  \n",
    "lr = 0.0030084157499084825, reg = 1e-10, validation accuracy is 0.386  \n",
    "lr = 0.0020421838641083946, reg = 1e-10, validation accuracy is 0.537  \n",
    "lr = 0.0014332955883302268, reg = 1e-10, validation accuracy is 0.553  \n",
    "lr = 0.0011608301485078573, reg = 1e-10, validation accuracy is 0.596  \n",
    "lr = 0.0008941778789603731, reg = 1e-10, validation accuracy is 0.613  \n",
    "lr = 0.0007377113977993159, reg = 1e-10, validation accuracy is 0.625  \n",
    "lr = 0.0006695355436664427, reg = 1e-10, validation accuracy is 0.667  \n",
    "lr = 0.00046849241080962356, reg = 1e-10, validation accuracy is 0.663  \n",
    "lr = 0.0004521058841800273, reg = 1e-10, validation accuracy is 0.687  \n",
    "lr = 0.0003554619545626375, reg = 1e-10, validation accuracy is 0.679  \n",
    "lr = 0.0003488919238298938, reg = 1e-10, validation accuracy is 0.699  \n",
    "lr = 0.0003106013879921905, reg = 1e-10, validation accuracy is 0.695  \n",
    "lr = 0.00013463548961918718, reg = 1e-10, validation accuracy is 0.697  \n",
    "lr = 0.00011263526559213542, reg = 1e-10, validation accuracy is 0.714  \n",
    "lr = 5.77437580640381e-05, reg = 1e-10, validation accuracy is 0.678  \n",
    "lr = 2.4948345072165963e-05, reg = 1e-10, validation accuracy is 0.656 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.0009616903078556671, reg = 21.272665634971855, validation accuracy is 0.079  \n",
      "lr = 0.0009616903078556671, reg = 3.704329527669045e-08, validation accuracy is 0.65  \n",
      "lr = 0.0009616903078556671, reg = 0.0007405454328093895, validation accuracy is 0.62  \n",
      "lr = 0.0009616903078556671, reg = 2.151419606920332e-06, validation accuracy is 0.616  \n",
      "lr = 0.0009616903078556671, reg = 0.00014211363631773827, validation accuracy is 0.65  \n",
      "lr = 0.0009616903078556671, reg = 1.6090791737988265e-06, validation accuracy is 0.649  \n",
      "lr = 0.0009616903078556671, reg = 3.477905750270434e-05, validation accuracy is 0.662  \n",
      "lr = 0.0009616903078556671, reg = 9.170751933898437e-10, validation accuracy is 0.632  \n",
      "lr = 0.0009616903078556671, reg = 1.946892102742311e-07, validation accuracy is 0.619  \n",
      "lr = 0.0009616903078556671, reg = 1.531242980758215e-06, validation accuracy is 0.666  \n",
      "Training time for 10 epochs: 168.34 sec\n",
      "In descending order of learning rate:\n",
      "lr = 0.0009616903078556671, reg = 21.272665634971855, validation accuracy is 0.079  \n",
      "lr = 0.0009616903078556671, reg = 3.704329527669045e-08, validation accuracy is 0.65  \n",
      "lr = 0.0009616903078556671, reg = 0.0007405454328093895, validation accuracy is 0.62  \n",
      "lr = 0.0009616903078556671, reg = 2.151419606920332e-06, validation accuracy is 0.616  \n",
      "lr = 0.0009616903078556671, reg = 0.00014211363631773827, validation accuracy is 0.65  \n",
      "lr = 0.0009616903078556671, reg = 1.6090791737988265e-06, validation accuracy is 0.649  \n",
      "lr = 0.0009616903078556671, reg = 3.477905750270434e-05, validation accuracy is 0.662  \n",
      "lr = 0.0009616903078556671, reg = 9.170751933898437e-10, validation accuracy is 0.632  \n",
      "lr = 0.0009616903078556671, reg = 1.946892102742311e-07, validation accuracy is 0.619  \n",
      "lr = 0.0009616903078556671, reg = 1.531242980758215e-06, validation accuracy is 0.666  \n"
     ]
    }
   ],
   "source": [
    "stat = []\n",
    "\n",
    "start = time.time()\n",
    "max_count = 10\n",
    "for count in range(max_count):\n",
    "    reg = 10**random.uniform(-10,2)\n",
    "    lr = 0.0009616903078556671\n",
    "    \n",
    "    model = generator().type(gpu_dtype)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=reg)\n",
    "    train(model, loss_fn, optimizer, num_epochs=1)\n",
    "    val_acc = check_accuracy(model, loader_val)\n",
    "    \n",
    "    print('lr = {}, reg = {}, validation accuracy is {}  '.format(lr, reg, val_acc))\n",
    "    stat.append([lr, reg, val_acc])\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "print('Training time for 10 epochs: {1:.2f} sec'.format(val_acc, end-start))\n",
    "\n",
    "sorted_stat = sorted(stat, key=lambda x: x[0], reverse=True)\n",
    "print (\"In descending order of learning rate:\")\n",
    "for lr, reg, val_acc in sorted_stat:\n",
    "    print('lr = {}, reg = {}, validation accuracy is {}  '.format(lr, reg, val_acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Learning Rate + Regularization\n",
    "\n",
    "####  Model 5-Conv Layers, 2 MaxPool Blocks (WIDER)\n",
    "\n",
    "Sequential(\n",
    "  (0): Conv2d (3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (1): ReLU(inplace)\n",
    "  (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (3): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (4): ReLU(inplace)\n",
    "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (6): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (7): ReLU(inplace)\n",
    "  (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
    "  (10): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (11): ReLU(inplace)\n",
    "  (12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (13): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "  (14): ReLU(inplace)\n",
    "  (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
    "  (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
    "  (17): Flatten(\n",
    "  )\n",
    "  (18): Linear(in_features=16384, out_features=1024)\n",
    "  (19): ReLU(inplace)\n",
    "  (20): Linear(in_features=1024, out_features=10)\n",
    ")\n",
    "\n",
    "* Training time for 1 epoch: 13.18 sec\n",
    "\n",
    "* In descending order of learning rate:\n",
    "lr = 0.0019448764802993351, reg = 0.0014559406583695056, validation accuracy is 0.64  \n",
    "lr = 0.0018436283136011092, reg = 1.20855073494988e-07, validation accuracy is 0.602  \n",
    "lr = 0.0017469433543108419, reg = 0.005407562156582751, validation accuracy is 0.588  \n",
    "lr = 0.0013869356779387546, reg = 5.94062005313746e-05, validation accuracy is 0.653  \n",
    "lr = 0.0012393288000769702, reg = 5.12602146030666, validation accuracy is 0.113  \n",
    "lr = 0.0008867992873069955, reg = 1.4372398781436804, validation accuracy is 0.199  \n",
    "lr = 0.0006996954263941067, reg = 1.9038337189684477e-06, validation accuracy is 0.708  \n",
    "** lr = 0.0006055829256460011, reg = 9.58996938648248e-07, validation accuracy is 0.722 **  \n",
    "** lr = 0.0005443722591657279, reg = 0.00017920546701173702, validation accuracy is 0.719 **  \n",
    "lr = 0.0005251670566001226, reg = 0.0018050589295523598, validation accuracy is 0.699  \n",
    "lr = 0.00048459438835051166, reg = 0.05983089061009524, validation accuracy is 0.621  \n",
    "lr = 0.0004420536106205546, reg = 2.0186822341150736e-06, validation accuracy is 0.768  \n",
    "** lr = 0.0004122334077427315, reg = 6.4451833938235595e-09, validation accuracy is 0.747 **  \n",
    "lr = 0.0003961550140640271, reg = 0.8105443384587647, validation accuracy is 0.37  \n",
    "lr = 0.00037206712670449505, reg = 0.09927454316034201, validation accuracy is 0.614  \n",
    "** lr = 0.000348507497041526, reg = 1.4605422936453535e-05, validation accuracy is 0.725 **  \n",
    "lr = 0.0003159819304478141, reg = 28.656846959736583, validation accuracy is 0.113  \n",
    "** lr = 0.00030976635881782815, reg = 1.9752562992451137e-05, validation accuracy is 0.737 **  \n",
    "** lr = 0.00030255947398111444, reg = 1.1887805824295586e-07, validation accuracy is 0.732 **   \n",
    "** lr = 0.0002938522880460647, reg = 3.1116344006152496e-09, validation accuracy is 0.731 **  \n",
    "<span style=\"color:blue\"> ** lr = 0.0002885154495256621, reg = 2.771973322549355e-09, validation accuracy is 0.75 ** <span style=\"color:black\">   \n",
    "lr = 0.0002719530062001166, reg = 8.181890417359118e-05, validation accuracy is 0.714  \n",
    "lr = 0.00025635190472579454, reg = 26.22154832223876, validation accuracy is 0.087  \n",
    "lr = 0.00016799852379122324, reg = 15.613566194573618, validation accuracy is 0.098  \n",
    "** lr = 0.0001638932273529234, reg = 3.0703157061644324e-05, validation accuracy is 0.73 **  \n",
    "** lr = 0.00016151175929450816, reg = 0.001461378511321383, validation accuracy is 0.73 **  \n",
    "lr = 0.00015862184981477027, reg = 3.311901217036856e-07, validation accuracy is 0.72  \n",
    "lr = 0.00014157475047081133, reg = 3.515875715049107e-05, validation accuracy is 0.723  \n",
    "lr = 0.00011805442164972787, reg = 3.818327291174253e-10, validation accuracy is 0.716  \n",
    "lr = 0.00011532989154129476, reg = 3.889665794272065, validation accuracy is 0.226  \n",
    "lr = 9.609980503912395e-05, reg = 0.005374903040295943, validation accuracy is 0.677  \n",
    "lr = 7.764263308560473e-05, reg = 3.9283309473615766, validation accuracy is 0.203  \n",
    "lr = 7.734499090723071e-05, reg = 2.6875164976126248e-09, validation accuracy is 0.681  \n",
    "lr = 6.914399074312584e-05, reg = 0.030651772897969106, validation accuracy is 0.686  \n",
    "lr = 5.341281559305708e-05, reg = 0.17541836465002894, validation accuracy is 0.632  \n",
    "lr = 5.268044462970665e-05, reg = 0.0948523510014383, validation accuracy is 0.668  \n",
    "lr = 4.971515443042102e-05, reg = 2.6175795871087453e-09, validation accuracy is 0.687  \n",
    "lr = 4.625603296907752e-05, reg = 6.0699709968522045e-09, validation accuracy is 0.666  \n",
    "lr = 4.007498928855568e-05, reg = 1.0462145053481942e-05, validation accuracy is 0.66  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 100\n",
      "t = 100, loss = 1.733967\n",
      "t = 200, loss = 1.499009\n",
      "t = 300, loss = 1.442712\n",
      "t = 400, loss = 1.046357\n",
      "t = 500, loss = 1.189031\n",
      "t = 600, loss = 1.171335\n",
      "train acc: 0.577025 val acc:0.5599\n",
      "t = 100, loss = 1.108519\n",
      "progress is as such: [0.577025, 0.5599, 1.4409123486242232, 1.2254601606191733]\n",
      "Starting epoch 2 / 100\n",
      "t = 100, loss = 0.818037\n",
      "t = 200, loss = 0.978447\n",
      "t = 300, loss = 1.078047\n",
      "t = 400, loss = 0.781222\n",
      "t = 500, loss = 0.809147\n",
      "t = 600, loss = 1.038934\n",
      "train acc: 0.740075 val acc:0.7061\n",
      "t = 100, loss = 0.767456\n",
      "progress is as such: [0.740075, 0.7061, 0.9820786821536529, 0.8466680474006213]\n",
      "Starting epoch 3 / 100\n",
      "t = 100, loss = 0.599189\n",
      "t = 200, loss = 0.786532\n",
      "t = 300, loss = 0.912590\n",
      "t = 400, loss = 0.587231\n",
      "t = 500, loss = 0.576157\n",
      "t = 600, loss = 0.774508\n",
      "train acc: 0.770525 val acc:0.7217\n",
      "t = 100, loss = 0.804989\n",
      "progress is as such: [0.770525, 0.7217, 0.7769515049667695, 0.8184727758933337]\n",
      "Starting epoch 4 / 100\n",
      "t = 100, loss = 0.469227\n",
      "t = 200, loss = 0.742791\n",
      "t = 300, loss = 0.615797\n",
      "t = 400, loss = 0.528767\n",
      "t = 500, loss = 0.466329\n",
      "t = 600, loss = 0.686970\n",
      "train acc: 0.8006 val acc:0.7418\n",
      "t = 100, loss = 0.808378\n",
      "progress is as such: [0.8006, 0.7418, 0.6472969601551691, 0.8045836646969502]\n",
      "Starting epoch 5 / 100\n",
      "t = 100, loss = 0.331559\n",
      "t = 200, loss = 0.434197\n",
      "t = 300, loss = 0.560065\n",
      "t = 400, loss = 0.474248\n",
      "t = 500, loss = 0.596161\n",
      "t = 600, loss = 0.462265\n",
      "train acc: 0.841825 val acc:0.7585\n",
      "t = 100, loss = 0.896574\n",
      "progress is as such: [0.841825, 0.7585, 0.5469229068272771, 0.769834310007401]\n",
      "Starting epoch 6 / 100\n",
      "t = 100, loss = 0.270954\n",
      "t = 200, loss = 0.474931\n",
      "t = 300, loss = 0.371715\n",
      "t = 400, loss = 0.264181\n",
      "t = 500, loss = 0.383600\n",
      "t = 600, loss = 0.261114\n",
      "train acc: 0.8421 val acc:0.7408\n",
      "t = 100, loss = 0.931106\n",
      "progress is as such: [0.8421, 0.7408, 0.4337706786269943, 0.9055057045740958]\n",
      "Starting epoch 7 / 100\n",
      "t = 100, loss = 0.238770\n",
      "t = 200, loss = 0.430189\n",
      "t = 300, loss = 0.216541\n",
      "t = 400, loss = 0.207888\n",
      "t = 500, loss = 0.426384\n",
      "t = 600, loss = 0.231164\n",
      "train acc: 0.8865 val acc:0.7663\n",
      "t = 100, loss = 0.761673\n",
      "progress is as such: [0.8865, 0.7663, 0.36586552827308577, 0.8203625409648969]\n",
      "Starting epoch 8 / 100\n",
      "t = 100, loss = 0.186368\n",
      "t = 200, loss = 0.275002\n",
      "t = 300, loss = 0.215502\n",
      "t = 400, loss = 0.205248\n",
      "t = 500, loss = 0.306000\n",
      "t = 600, loss = 0.100946\n",
      "train acc: 0.903825 val acc:0.7691\n",
      "t = 100, loss = 0.940072\n",
      "progress is as such: [0.903825, 0.7691, 0.3070288436033596, 0.8726877061984478]\n",
      "Starting epoch 9 / 100\n",
      "t = 100, loss = 0.184975\n",
      "t = 200, loss = 0.226544\n",
      "t = 300, loss = 0.413190\n",
      "t = 400, loss = 0.223681\n",
      "t = 500, loss = 0.344477\n",
      "t = 600, loss = 0.219525\n",
      "train acc: 0.906 val acc:0.7658\n",
      "t = 100, loss = 0.972525\n",
      "progress is as such: [0.906, 0.7658, 0.26619214556800824, 0.8533207453214205]\n",
      "Starting epoch 10 / 100\n",
      "t = 100, loss = 0.246681\n",
      "t = 200, loss = 0.265186\n",
      "t = 300, loss = 0.149113\n",
      "t = 400, loss = 0.170637\n",
      "t = 500, loss = 0.267167\n",
      "t = 600, loss = 0.175640\n",
      "train acc: 0.91695 val acc:0.7643\n",
      "t = 100, loss = 0.714832\n",
      "progress is as such: [0.91695, 0.7643, 0.23095727753109083, 0.8909018167700523]\n",
      "Starting epoch 11 / 100\n",
      "t = 100, loss = 0.082617\n",
      "t = 200, loss = 0.085574\n",
      "t = 300, loss = 0.137728\n",
      "t = 400, loss = 0.155456\n",
      "t = 500, loss = 0.080933\n",
      "t = 600, loss = 0.077420\n",
      "train acc: 0.94115 val acc:0.7757\n",
      "t = 100, loss = 1.002717\n",
      "progress is as such: [0.94115, 0.7757, 0.1850023054643176, 0.9184682317651235]\n",
      "Starting epoch 12 / 100\n",
      "t = 100, loss = 0.070127\n",
      "t = 200, loss = 0.116166\n",
      "t = 300, loss = 0.187650\n",
      "t = 400, loss = 0.091579\n",
      "t = 500, loss = 0.116958\n",
      "t = 600, loss = 0.197214\n",
      "train acc: 0.9434 val acc:0.7727\n",
      "t = 100, loss = 1.037150\n",
      "progress is as such: [0.9434, 0.7727, 0.15529299034283328, 0.9516917822452692]\n",
      "Starting epoch 13 / 100\n",
      "t = 100, loss = 0.079952\n",
      "t = 200, loss = 0.182858\n",
      "t = 300, loss = 0.134488\n",
      "t = 400, loss = 0.109948\n",
      "t = 500, loss = 0.094873\n",
      "t = 600, loss = 0.074700\n",
      "train acc: 0.935225 val acc:0.7627\n",
      "t = 100, loss = 0.841550\n",
      "progress is as such: [0.935225, 0.7627, 0.14910628092893136, 0.9628814118794906]\n",
      "Starting epoch 14 / 100\n",
      "t = 100, loss = 0.128246\n",
      "t = 200, loss = 0.128479\n",
      "t = 300, loss = 0.088839\n",
      "t = 400, loss = 0.188085\n",
      "t = 500, loss = 0.123003\n",
      "t = 600, loss = 0.159065\n",
      "train acc: 0.95385 val acc:0.7809\n",
      "t = 100, loss = 1.119265\n",
      "progress is as such: [0.95385, 0.7809, 0.13836496332600617, 0.8988244681595228]\n",
      "Starting epoch 15 / 100\n",
      "t = 100, loss = 0.128436\n",
      "t = 200, loss = 0.088138\n",
      "t = 300, loss = 0.057169\n",
      "t = 400, loss = 0.037647\n",
      "t = 500, loss = 0.072143\n",
      "t = 600, loss = 0.216715\n",
      "train acc: 0.95235 val acc:0.7788\n",
      "t = 100, loss = 1.119523\n",
      "progress is as such: [0.95235, 0.7788, 0.1271141228469041, 0.9591747679007359]\n",
      "Starting epoch 16 / 100\n",
      "t = 100, loss = 0.072649\n",
      "t = 200, loss = 0.078528\n",
      "t = 300, loss = 0.087871\n",
      "t = 400, loss = 0.059323\n",
      "t = 500, loss = 0.075545\n",
      "t = 600, loss = 0.200949\n",
      "train acc: 0.960175 val acc:0.7783\n",
      "t = 100, loss = 1.325398\n",
      "progress is as such: [0.960175, 0.7783, 0.10793288032679509, 0.9630652721493672]\n",
      "Starting epoch 17 / 100\n",
      "t = 100, loss = 0.049815\n",
      "t = 200, loss = 0.088071\n",
      "t = 300, loss = 0.095194\n",
      "t = 400, loss = 0.056801\n",
      "t = 500, loss = 0.056156\n",
      "t = 600, loss = 0.016230\n",
      "train acc: 0.9678 val acc:0.7861\n",
      "t = 100, loss = 1.286633\n",
      "progress is as such: [0.9678, 0.7861, 0.09929827787280561, 0.9409793831216984]\n",
      "Starting epoch 18 / 100\n",
      "t = 100, loss = 0.155660\n",
      "t = 200, loss = 0.047294\n",
      "t = 300, loss = 0.124790\n",
      "t = 400, loss = 0.079734\n",
      "t = 500, loss = 0.150725\n",
      "t = 600, loss = 0.028618\n",
      "train acc: 0.965325 val acc:0.787\n",
      "t = 100, loss = 1.122280\n",
      "progress is as such: [0.965325, 0.787, 0.09583319240930276, 0.9618905656612836]\n",
      "Starting epoch 19 / 100\n",
      "t = 100, loss = 0.107588\n",
      "t = 200, loss = 0.088778\n",
      "t = 300, loss = 0.042796\n",
      "t = 400, loss = 0.075814\n",
      "t = 500, loss = 0.049115\n",
      "t = 600, loss = 0.131638\n",
      "train acc: 0.96195 val acc:0.7817\n",
      "t = 100, loss = 1.130005\n",
      "progress is as such: [0.96195, 0.7817, 0.10102558499261832, 0.9673759421476951]\n",
      "Starting epoch 20 / 100\n",
      "t = 100, loss = 0.102054\n",
      "t = 200, loss = 0.021718\n",
      "t = 300, loss = 0.126124\n",
      "t = 400, loss = 0.122127\n",
      "t = 500, loss = 0.273384\n",
      "t = 600, loss = 0.109061\n",
      "train acc: 0.96295 val acc:0.7817\n",
      "t = 100, loss = 1.291872\n",
      "progress is as such: [0.96295, 0.7817, 0.09737108146640448, 0.9503342220798517]\n",
      "Starting epoch 21 / 100\n",
      "t = 100, loss = 0.066901\n",
      "t = 200, loss = 0.054422\n",
      "t = 300, loss = 0.043218\n",
      "t = 400, loss = 0.108119\n",
      "t = 500, loss = 0.012700\n",
      "t = 600, loss = 0.064600\n",
      "train acc: 0.970425 val acc:0.7827\n",
      "t = 100, loss = 1.576919\n",
      "progress is as such: [0.970425, 0.7827, 0.07437107061597113, 0.9978334645812328]\n",
      "Starting epoch 22 / 100\n",
      "t = 100, loss = 0.065184\n",
      "t = 200, loss = 0.048273\n",
      "t = 300, loss = 0.013192\n",
      "t = 400, loss = 0.010119\n",
      "t = 500, loss = 0.133287\n",
      "t = 600, loss = 0.104257\n",
      "train acc: 0.96965 val acc:0.784\n",
      "t = 100, loss = 1.506722\n",
      "progress is as such: [0.96965, 0.784, 0.07388281047594948, 0.9932241842914851]\n",
      "Starting epoch 23 / 100\n",
      "t = 100, loss = 0.044183\n",
      "t = 200, loss = 0.228960\n",
      "t = 300, loss = 0.037989\n",
      "t = 400, loss = 0.103814\n",
      "t = 500, loss = 0.075247\n",
      "t = 600, loss = 0.045524\n",
      "train acc: 0.969925 val acc:0.7848\n",
      "t = 100, loss = 1.043412\n",
      "progress is as such: [0.969925, 0.7848, 0.07920625564964631, 0.9855597557929846]\n",
      "Starting epoch 24 / 100\n",
      "t = 100, loss = 0.006410\n",
      "t = 200, loss = 0.089782\n",
      "t = 300, loss = 0.031947\n",
      "t = 400, loss = 0.038004\n",
      "t = 500, loss = 0.050961\n",
      "t = 600, loss = 0.051685\n",
      "train acc: 0.971175 val acc:0.794\n",
      "t = 100, loss = 1.194136\n",
      "progress is as such: [0.971175, 0.794, 0.08145539787335274, 0.983009689511397]\n",
      "Starting epoch 25 / 100\n",
      "t = 100, loss = 0.044898\n",
      "t = 200, loss = 0.129399\n",
      "t = 300, loss = 0.108547\n",
      "t = 400, loss = 0.025466\n",
      "t = 500, loss = 0.073335\n",
      "t = 600, loss = 0.056941\n",
      "train acc: 0.974125 val acc:0.7856\n",
      "t = 100, loss = 1.313946\n",
      "progress is as such: [0.974125, 0.7856, 0.07515886366271819, 1.0014233562426689]\n",
      "Starting epoch 26 / 100\n",
      "t = 100, loss = 0.021680\n",
      "t = 200, loss = 0.018350\n",
      "t = 300, loss = 0.031349\n",
      "t = 400, loss = 0.107542\n",
      "t = 500, loss = 0.011451\n",
      "t = 600, loss = 0.039083\n",
      "train acc: 0.97605 val acc:0.7858\n",
      "t = 100, loss = 1.322788\n",
      "progress is as such: [0.97605, 0.7858, 0.056785650157298036, 1.0403610955064113]\n",
      "Starting epoch 27 / 100\n",
      "t = 100, loss = 0.046575\n",
      "t = 200, loss = 0.012738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 300, loss = 0.092238\n",
      "t = 400, loss = 0.050331\n",
      "t = 500, loss = 0.045821\n",
      "t = 600, loss = 0.096687\n",
      "train acc: 0.9702 val acc:0.783\n",
      "t = 100, loss = 1.476754\n",
      "progress is as such: [0.9702, 0.783, 0.05969398693503955, 1.032732418141304]\n",
      "Starting epoch 28 / 100\n",
      "t = 100, loss = 0.035534\n",
      "t = 200, loss = 0.066996\n",
      "t = 300, loss = 0.064119\n",
      "t = 400, loss = 0.070251\n",
      "t = 500, loss = 0.054890\n",
      "t = 600, loss = 0.015371\n",
      "train acc: 0.97525 val acc:0.7878\n",
      "t = 100, loss = 1.512495\n",
      "progress is as such: [0.97525, 0.7878, 0.06457480733008243, 1.0146078799779599]\n",
      "Starting epoch 29 / 100\n",
      "t = 100, loss = 0.115408\n",
      "t = 200, loss = 0.105954\n",
      "t = 300, loss = 0.054194\n",
      "t = 400, loss = 0.036183\n",
      "t = 500, loss = 0.070292\n",
      "t = 600, loss = 0.024560\n",
      "train acc: 0.977225 val acc:0.7889\n",
      "t = 100, loss = 1.312698\n",
      "progress is as such: [0.977225, 0.7889, 0.06610587171207254, 0.9784626846130078]\n",
      "Starting epoch 30 / 100\n",
      "t = 100, loss = 0.062534\n",
      "t = 200, loss = 0.039131\n",
      "t = 300, loss = 0.137659\n",
      "t = 400, loss = 0.009794\n",
      "t = 500, loss = 0.063243\n",
      "t = 600, loss = 0.012364\n",
      "train acc: 0.970225 val acc:0.7846\n",
      "t = 100, loss = 1.100639\n",
      "progress is as such: [0.970225, 0.7846, 0.06472659022368205, 1.0176904262640538]\n",
      "Starting epoch 31 / 100\n",
      "t = 100, loss = 0.021361\n",
      "t = 200, loss = 0.018135\n",
      "t = 300, loss = 0.026517\n",
      "t = 400, loss = 0.010458\n",
      "t = 500, loss = 0.036674\n",
      "t = 600, loss = 0.012305\n",
      "train acc: 0.98065 val acc:0.7888\n",
      "t = 100, loss = 1.459334\n",
      "progress is as such: [0.98065, 0.7888, 0.04923261845341095, 1.0127118854568555]\n",
      "Starting epoch 32 / 100\n",
      "t = 100, loss = 0.019290\n",
      "t = 200, loss = 0.046769\n",
      "t = 300, loss = 0.141509\n",
      "t = 400, loss = 0.079163\n",
      "t = 500, loss = 0.017817\n",
      "t = 600, loss = 0.148734\n",
      "train acc: 0.9797 val acc:0.7826\n",
      "t = 100, loss = 1.437678\n",
      "progress is as such: [0.9797, 0.7826, 0.049193157942798466, 1.0533088520169258]\n",
      "Starting epoch 33 / 100\n",
      "t = 100, loss = 0.011627\n",
      "t = 200, loss = 0.131380\n",
      "t = 300, loss = 0.028117\n",
      "t = 400, loss = 0.009542\n",
      "t = 500, loss = 0.045605\n",
      "t = 600, loss = 0.227508\n",
      "train acc: 0.980125 val acc:0.7922\n",
      "t = 100, loss = 1.240647\n",
      "progress is as such: [0.980125, 0.7922, 0.05421236617108568, 0.9945417820261075]\n",
      "Starting epoch 34 / 100\n",
      "t = 100, loss = 0.016831\n",
      "t = 200, loss = 0.072562\n",
      "t = 300, loss = 0.012848\n",
      "t = 400, loss = 0.022087\n",
      "t = 500, loss = 0.091265\n",
      "t = 600, loss = 0.063607\n",
      "train acc: 0.982325 val acc:0.7938\n",
      "t = 100, loss = 1.209846\n",
      "progress is as such: [0.982325, 0.7938, 0.05796296819328116, 0.9933299880761367]\n",
      "Starting epoch 35 / 100\n",
      "t = 100, loss = 0.031328\n",
      "t = 200, loss = 0.045168\n",
      "t = 300, loss = 0.008595\n",
      "t = 400, loss = 0.042664\n",
      "t = 500, loss = 0.027954\n",
      "t = 600, loss = 0.034125\n",
      "train acc: 0.979075 val acc:0.7893\n",
      "t = 100, loss = 1.221249\n",
      "progress is as such: [0.979075, 0.7893, 0.05082781507203785, 1.0333465219308169]\n",
      "Starting epoch 36 / 100\n",
      "t = 100, loss = 0.013598\n",
      "t = 200, loss = 0.017739\n",
      "t = 300, loss = 0.003507\n",
      "t = 400, loss = 0.020033\n",
      "t = 500, loss = 0.016267\n",
      "t = 600, loss = 0.043179\n",
      "train acc: 0.98605 val acc:0.7907\n",
      "t = 100, loss = 1.246104\n",
      "progress is as such: [0.98605, 0.7907, 0.038614260322318815, 1.03092138029826]\n",
      "Starting epoch 37 / 100\n",
      "t = 100, loss = 0.055984\n",
      "t = 200, loss = 0.107346\n",
      "t = 300, loss = 0.035819\n",
      "t = 400, loss = 0.025657\n",
      "t = 500, loss = 0.088457\n",
      "t = 600, loss = 0.028478\n",
      "train acc: 0.977875 val acc:0.7837\n",
      "t = 100, loss = 1.330997\n",
      "progress is as such: [0.977875, 0.7837, 0.03635708286235921, 1.0634185079580698]\n",
      "Starting epoch 38 / 100\n",
      "t = 100, loss = 0.070824\n",
      "t = 200, loss = 0.049213\n",
      "t = 300, loss = 0.064000\n",
      "t = 400, loss = 0.046082\n",
      "t = 500, loss = 0.073762\n",
      "t = 600, loss = 0.003862\n",
      "train acc: 0.976875 val acc:0.7801\n",
      "t = 100, loss = 0.895096\n",
      "progress is as such: [0.976875, 0.7801, 0.052385870018042624, 1.07816820171399]\n",
      "Starting epoch 39 / 100\n",
      "t = 100, loss = 0.046640\n",
      "t = 200, loss = 0.040268\n",
      "t = 300, loss = 0.154550\n",
      "t = 400, loss = 0.084108\n",
      "t = 500, loss = 0.052172\n",
      "t = 600, loss = 0.085501\n",
      "train acc: 0.981525 val acc:0.7819\n",
      "t = 100, loss = 1.106541\n",
      "progress is as such: [0.981525, 0.7819, 0.04151650321168395, 1.076117845873038]\n",
      "Starting epoch 40 / 100\n",
      "t = 100, loss = 0.006432\n",
      "t = 200, loss = 0.146195\n",
      "t = 300, loss = 0.165785\n",
      "t = 400, loss = 0.026836\n",
      "t = 500, loss = 0.082920\n",
      "t = 600, loss = 0.062486\n",
      "train acc: 0.97995 val acc:0.7869\n",
      "t = 100, loss = 0.996554\n",
      "progress is as such: [0.97995, 0.7869, 0.04638790851757408, 1.0588835139687245]\n",
      "Starting epoch 41 / 100\n",
      "t = 100, loss = 0.004904\n",
      "t = 200, loss = 0.009176\n",
      "t = 300, loss = 0.030499\n",
      "t = 400, loss = 0.032599\n",
      "t = 500, loss = 0.009561\n",
      "t = 600, loss = 0.043030\n",
      "train acc: 0.988625 val acc:0.7896\n",
      "t = 100, loss = 1.001630\n",
      "progress is as such: [0.988625, 0.7896, 0.03499734738411812, 1.0711059427032104]\n",
      "Starting epoch 42 / 100\n",
      "t = 100, loss = 0.094877\n",
      "t = 200, loss = 0.014514\n",
      "t = 300, loss = 0.009751\n",
      "t = 400, loss = 0.034237\n",
      "t = 500, loss = 0.002668\n",
      "t = 600, loss = 0.021604\n",
      "train acc: 0.98225 val acc:0.7865\n",
      "t = 100, loss = 1.223429\n",
      "progress is as such: [0.98225, 0.7865, 0.02910866893123453, 1.0902103260159492]\n",
      "Starting epoch 43 / 100\n",
      "t = 100, loss = 0.002511\n",
      "t = 200, loss = 0.090918\n",
      "t = 300, loss = 0.004090\n",
      "t = 400, loss = 0.004587\n",
      "t = 500, loss = 0.007465\n",
      "t = 600, loss = 0.063164\n",
      "train acc: 0.98795 val acc:0.7906\n",
      "t = 100, loss = 1.194181\n",
      "progress is as such: [0.98795, 0.7906, 0.04184730156348684, 1.050146493010032]\n",
      "Starting epoch 44 / 100\n",
      "t = 100, loss = 0.023222\n",
      "t = 200, loss = 0.040219\n",
      "t = 300, loss = 0.004704\n",
      "t = 400, loss = 0.057589\n",
      "t = 500, loss = 0.016877\n",
      "t = 600, loss = 0.064731\n",
      "train acc: 0.98545 val acc:0.7876\n",
      "t = 100, loss = 1.200763\n",
      "progress is as such: [0.98545, 0.7876, 0.036670129522323035, 1.0623985670315914]\n",
      "Starting epoch 45 / 100\n",
      "t = 100, loss = 0.006799\n",
      "t = 200, loss = 0.013533\n",
      "t = 300, loss = 0.014687\n",
      "t = 400, loss = 0.005078\n",
      "t = 500, loss = 0.042015\n",
      "t = 600, loss = 0.013147\n",
      "train acc: 0.985375 val acc:0.7917\n",
      "t = 100, loss = 1.245337\n",
      "progress is as such: [0.985375, 0.7917, 0.04063592231175743, 1.0472516833971708]\n",
      "Starting epoch 46 / 100\n",
      "t = 100, loss = 0.002296\n",
      "t = 200, loss = 0.003364\n",
      "t = 300, loss = 0.008366\n",
      "t = 400, loss = 0.009674\n",
      "t = 500, loss = 0.011808\n",
      "t = 600, loss = 0.032070\n",
      "train acc: 0.991275 val acc:0.7997\n",
      "t = 100, loss = 1.241987\n",
      "progress is as such: [0.991275, 0.7997, 0.027444886312318537, 1.0162998111202166]\n",
      "Starting epoch 47 / 100\n",
      "t = 100, loss = 0.016471\n",
      "t = 200, loss = 0.009492\n",
      "t = 300, loss = 0.019376\n",
      "t = 400, loss = 0.008493\n",
      "t = 500, loss = 0.061443\n",
      "t = 600, loss = 0.006002\n",
      "train acc: 0.981775 val acc:0.7842\n",
      "t = 100, loss = 1.414835\n",
      "progress is as such: [0.981775, 0.7842, 0.024034049773875337, 1.1277648319418614]\n",
      "Starting epoch 48 / 100\n",
      "t = 100, loss = 0.011086\n",
      "t = 200, loss = 0.003057\n",
      "t = 300, loss = 0.026345\n",
      "t = 400, loss = 0.044210\n",
      "t = 500, loss = 0.006528\n",
      "t = 600, loss = 0.063087\n",
      "train acc: 0.985575 val acc:0.7879\n",
      "t = 100, loss = 1.590164\n",
      "progress is as such: [0.985575, 0.7879, 0.03700641428347295, 1.0568857727906642]\n",
      "Starting epoch 49 / 100\n",
      "t = 100, loss = 0.001176\n",
      "t = 200, loss = 0.007819\n",
      "t = 300, loss = 0.021483\n",
      "t = 400, loss = 0.014301\n",
      "t = 500, loss = 0.012062\n",
      "t = 600, loss = 0.021622\n",
      "train acc: 0.984075 val acc:0.7875\n",
      "t = 100, loss = 1.550444\n",
      "progress is as such: [0.984075, 0.7875, 0.032835186641209595, 1.0807194851147823]\n",
      "Starting epoch 50 / 100\n",
      "t = 100, loss = 0.034965\n",
      "t = 200, loss = 0.010869\n",
      "t = 300, loss = 0.031964\n",
      "t = 400, loss = 0.009839\n",
      "t = 500, loss = 0.062582\n",
      "t = 600, loss = 0.011224\n",
      "train acc: 0.989425 val acc:0.7931\n",
      "t = 100, loss = 1.409526\n",
      "progress is as such: [0.989425, 0.7931, 0.02690460447532435, 1.0617322434599583]\n",
      "Starting epoch 51 / 100\n",
      "t = 100, loss = 0.014446\n",
      "t = 200, loss = 0.017220\n",
      "t = 300, loss = 0.049242\n",
      "t = 400, loss = 0.001916\n",
      "t = 500, loss = 0.006895\n",
      "t = 600, loss = 0.027551\n",
      "train acc: 0.991025 val acc:0.7985\n",
      "t = 100, loss = 1.312698\n",
      "progress is as such: [0.991025, 0.7985, 0.02327848890593323, 1.0303170203398435]\n",
      "Starting epoch 52 / 100\n",
      "t = 100, loss = 0.008199\n",
      "t = 200, loss = 0.009157\n",
      "t = 300, loss = 0.033180\n",
      "t = 400, loss = 0.001564\n",
      "t = 500, loss = 0.024306\n",
      "t = 600, loss = 0.022518\n",
      "train acc: 0.9885 val acc:0.7933\n",
      "t = 100, loss = 1.057778\n",
      "progress is as such: [0.9885, 0.7933, 0.02798221487766848, 1.034373144690807]\n",
      "Starting epoch 53 / 100\n",
      "t = 100, loss = 0.007297\n",
      "t = 200, loss = 0.008408\n",
      "t = 300, loss = 0.005239\n",
      "t = 400, loss = 0.047503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 500, loss = 0.059463\n",
      "t = 600, loss = 0.000801\n",
      "train acc: 0.99075 val acc:0.7953\n",
      "t = 100, loss = 1.013182\n",
      "progress is as such: [0.99075, 0.7953, 0.023574720402487006, 1.0567349795347605]\n",
      "Starting epoch 54 / 100\n",
      "t = 100, loss = 0.022311\n",
      "t = 200, loss = 0.001492\n",
      "t = 300, loss = 0.141616\n",
      "t = 400, loss = 0.049643\n",
      "t = 500, loss = 0.004740\n",
      "t = 600, loss = 0.001091\n",
      "train acc: 0.9904 val acc:0.7905\n",
      "t = 100, loss = 0.920901\n",
      "progress is as such: [0.9904, 0.7905, 0.02369764493778348, 1.0867483237614999]\n",
      "Starting epoch 55 / 100\n",
      "t = 100, loss = 0.005383\n",
      "t = 200, loss = 0.014165\n",
      "t = 300, loss = 0.030660\n",
      "t = 400, loss = 0.041236\n",
      "t = 500, loss = 0.080276\n",
      "t = 600, loss = 0.032289\n",
      "train acc: 0.987825 val acc:0.7862\n",
      "t = 100, loss = 1.382983\n",
      "progress is as such: [0.987825, 0.7862, 0.03511703141833632, 1.051961555503882]\n",
      "Starting epoch 56 / 100\n",
      "t = 100, loss = 0.002521\n",
      "t = 200, loss = 0.041773\n",
      "t = 300, loss = 0.001130\n",
      "t = 400, loss = 0.009374\n",
      "t = 500, loss = 0.007416\n",
      "t = 600, loss = 0.001644\n",
      "train acc: 0.996175 val acc:0.7985\n",
      "t = 100, loss = 1.136770\n",
      "progress is as such: [0.996175, 0.7985, 0.017524279824171502, 1.031553413432378]\n",
      "Starting epoch 57 / 100\n",
      "t = 100, loss = 0.009332\n",
      "t = 200, loss = 0.002457\n",
      "t = 300, loss = 0.003492\n",
      "t = 400, loss = 0.000830\n",
      "t = 500, loss = 0.003354\n",
      "t = 600, loss = 0.005670\n",
      "train acc: 0.9953 val acc:0.7995\n",
      "t = 100, loss = 1.076864\n",
      "progress is as such: [0.9953, 0.7995, 0.010534448725266907, 1.0677493931009219]\n",
      "Starting epoch 58 / 100\n",
      "t = 100, loss = 0.075453\n",
      "t = 200, loss = 0.001476\n",
      "t = 300, loss = 0.010492\n",
      "t = 400, loss = 0.052025\n",
      "t = 500, loss = 0.015948\n",
      "t = 600, loss = 0.013130\n",
      "train acc: 0.983425 val acc:0.7871\n",
      "t = 100, loss = 1.259607\n",
      "progress is as such: [0.983425, 0.7871, 0.018785013142638862, 1.1250416380472672]\n",
      "Starting epoch 59 / 100\n",
      "t = 100, loss = 0.037212\n",
      "t = 200, loss = 0.072155\n",
      "t = 300, loss = 0.022889\n",
      "t = 400, loss = 0.059669\n",
      "t = 500, loss = 0.006063\n",
      "t = 600, loss = 0.054008\n",
      "train acc: 0.991525 val acc:0.7949\n",
      "t = 100, loss = 0.973514\n",
      "progress is as such: [0.991525, 0.7949, 0.04076272640854885, 1.0072566179128795]\n",
      "Starting epoch 60 / 100\n",
      "t = 100, loss = 0.009891\n",
      "t = 200, loss = 0.033646\n",
      "t = 300, loss = 0.134967\n",
      "t = 400, loss = 0.002706\n",
      "t = 500, loss = 0.007993\n",
      "t = 600, loss = 0.002537\n",
      "train acc: 0.994975 val acc:0.7981\n",
      "t = 100, loss = 0.983651\n",
      "progress is as such: [0.994975, 0.7981, 0.017734373370424297, 1.0300660863136635]\n",
      "Starting epoch 61 / 100\n",
      "t = 100, loss = 0.009146\n",
      "t = 200, loss = 0.012483\n",
      "t = 300, loss = 0.008541\n",
      "t = 400, loss = 0.006103\n",
      "t = 500, loss = 0.002437\n",
      "t = 600, loss = 0.023034\n",
      "train acc: 0.9973 val acc:0.8024\n",
      "t = 100, loss = 1.019651\n",
      "progress is as such: [0.9973, 0.8024, 0.009829956569517843, 1.0302078028519948]\n",
      "Starting epoch 62 / 100\n",
      "t = 100, loss = 0.002452\n",
      "t = 200, loss = 0.001840\n",
      "t = 300, loss = 0.005964\n",
      "t = 400, loss = 0.002059\n",
      "t = 500, loss = 0.041564\n",
      "t = 600, loss = 0.004176\n",
      "train acc: 0.990525 val acc:0.7917\n",
      "t = 100, loss = 1.203873\n",
      "progress is as such: [0.990525, 0.7917, 0.009937502455730468, 1.1177755105189788]\n",
      "Starting epoch 63 / 100\n",
      "t = 100, loss = 0.005698\n",
      "t = 200, loss = 0.003866\n",
      "t = 300, loss = 0.018315\n",
      "t = 400, loss = 0.011586\n",
      "t = 500, loss = 0.136055\n",
      "t = 600, loss = 0.039910\n",
      "train acc: 0.99005 val acc:0.7933\n",
      "t = 100, loss = 1.133083\n",
      "progress is as such: [0.99005, 0.7933, 0.03336851304480567, 1.0629259187441606]\n",
      "Starting epoch 64 / 100\n",
      "t = 100, loss = 0.143059\n",
      "t = 200, loss = 0.010034\n",
      "t = 300, loss = 0.088607\n",
      "t = 400, loss = 0.032013\n",
      "t = 500, loss = 0.065936\n",
      "t = 600, loss = 0.022983\n",
      "train acc: 0.991975 val acc:0.7958\n",
      "t = 100, loss = 1.328924\n",
      "progress is as such: [0.991975, 0.7958, 0.02311255861945355, 1.0681135476781771]\n",
      "Starting epoch 65 / 100\n",
      "t = 100, loss = 0.057123\n",
      "t = 200, loss = 0.015664\n",
      "t = 300, loss = 0.003481\n",
      "t = 400, loss = 0.008311\n",
      "t = 500, loss = 0.001771\n",
      "t = 600, loss = 0.036922\n",
      "train acc: 0.991025 val acc:0.7968\n",
      "t = 100, loss = 1.460422\n",
      "progress is as such: [0.991025, 0.7968, 0.013550486487264816, 1.0995708560714355]\n",
      "Starting epoch 66 / 100\n",
      "t = 100, loss = 0.001419\n",
      "t = 200, loss = 0.002761\n",
      "t = 300, loss = 0.002971\n",
      "t = 400, loss = 0.132955\n",
      "t = 500, loss = 0.000942\n",
      "t = 600, loss = 0.001330\n",
      "train acc: 0.99555 val acc:0.8002\n",
      "t = 100, loss = 1.157959\n",
      "progress is as such: [0.99555, 0.8002, 0.0122048574535606, 1.0641541295708754]\n",
      "Starting epoch 67 / 100\n",
      "t = 100, loss = 0.001814\n",
      "t = 200, loss = 0.010495\n",
      "t = 300, loss = 0.002164\n",
      "t = 400, loss = 0.017477\n",
      "t = 500, loss = 0.020147\n",
      "t = 600, loss = 0.000910\n",
      "train acc: 0.9943 val acc:0.7981\n",
      "t = 100, loss = 1.316005\n",
      "progress is as such: [0.9943, 0.7981, 0.008909990363276731, 1.0915846429192102]\n",
      "Starting epoch 68 / 100\n",
      "t = 100, loss = 0.000447\n",
      "t = 200, loss = 0.002589\n",
      "t = 300, loss = 0.029123\n",
      "t = 400, loss = 0.003158\n",
      "t = 500, loss = 0.012664\n",
      "t = 600, loss = 0.005940\n",
      "train acc: 0.990375 val acc:0.794\n",
      "t = 100, loss = 1.064421\n",
      "progress is as such: [0.990375, 0.794, 0.021713441724363618, 1.1003710123208852]\n",
      "Starting epoch 69 / 100\n",
      "t = 100, loss = 0.070034\n",
      "t = 200, loss = 0.006612\n",
      "t = 300, loss = 0.013295\n",
      "t = 400, loss = 0.014864\n",
      "t = 500, loss = 0.003027\n",
      "t = 600, loss = 0.001492\n",
      "train acc: 0.992225 val acc:0.7942\n",
      "t = 100, loss = 1.142306\n",
      "progress is as such: [0.992225, 0.7942, 0.01682900658581788, 1.1220170332071109]\n",
      "Starting epoch 70 / 100\n",
      "t = 100, loss = 0.001243\n",
      "t = 200, loss = 0.038483\n",
      "t = 300, loss = 0.003831\n",
      "t = 400, loss = 0.097633\n",
      "t = 500, loss = 0.011154\n",
      "t = 600, loss = 0.008214\n",
      "train acc: 0.992325 val acc:0.795\n",
      "t = 100, loss = 1.105651\n",
      "progress is as such: [0.992325, 0.795, 0.01803826886969499, 1.0844643639448361]\n",
      "Starting epoch 71 / 100\n",
      "t = 100, loss = 0.019541\n",
      "t = 200, loss = 0.010157\n",
      "t = 300, loss = 0.002530\n",
      "t = 400, loss = 0.003981\n",
      "t = 500, loss = 0.009585\n",
      "t = 600, loss = 0.003044\n",
      "train acc: 0.996575 val acc:0.7942\n",
      "t = 100, loss = 1.236917\n",
      "progress is as such: [0.996575, 0.7942, 0.013248766366487894, 1.0813603878785403]\n",
      "Starting epoch 72 / 100\n",
      "t = 100, loss = 0.006786\n",
      "t = 200, loss = 0.001535\n",
      "t = 300, loss = 0.001032\n",
      "t = 400, loss = 0.008611\n",
      "t = 500, loss = 0.001712\n",
      "t = 600, loss = 0.002313\n",
      "train acc: 0.997675 val acc:0.8038\n",
      "t = 100, loss = 1.168826\n",
      "progress is as such: [0.997675, 0.8038, 0.00678780736724058, 1.0563813069692025]\n",
      "Starting epoch 73 / 100\n",
      "t = 100, loss = 0.002749\n",
      "t = 200, loss = 0.001086\n",
      "t = 300, loss = 0.001793\n",
      "t = 400, loss = 0.001067\n",
      "t = 500, loss = 0.001873\n",
      "t = 600, loss = 0.000945\n",
      "train acc: 0.99955 val acc:0.8091\n",
      "t = 100, loss = 1.239105\n",
      "progress is as such: [0.99955, 0.8091, 0.0026508679864211725, 1.007608561561658]\n",
      "Starting epoch 74 / 100\n",
      "t = 100, loss = 0.000567\n",
      "t = 200, loss = 0.000845\n",
      "t = 300, loss = 0.001002\n",
      "t = 400, loss = 0.000694\n",
      "t = 500, loss = 0.000975\n",
      "t = 600, loss = 0.000803\n",
      "train acc: 0.99975 val acc:0.8132\n",
      "t = 100, loss = 1.192216\n",
      "progress is as such: [0.99975, 0.8132, 0.0009179625851221574, 0.9501042146331224]\n",
      "Starting epoch 75 / 100\n",
      "t = 100, loss = 0.000710\n",
      "t = 200, loss = 0.001028\n",
      "t = 300, loss = 0.001077\n",
      "t = 400, loss = 0.000792\n",
      "t = 500, loss = 0.001017\n",
      "t = 600, loss = 0.000999\n",
      "train acc: 0.99985 val acc:0.8133\n",
      "t = 100, loss = 1.124937\n",
      "progress is as such: [0.99985, 0.8133, 0.0008900696172928199, 0.9118129979723539]\n",
      "Starting epoch 76 / 100\n",
      "t = 100, loss = 0.000847\n",
      "t = 200, loss = 0.001018\n",
      "t = 300, loss = 0.001125\n",
      "t = 400, loss = 0.000820\n",
      "t = 500, loss = 0.001038\n",
      "t = 600, loss = 0.000967\n",
      "train acc: 0.999875 val acc:0.8118\n",
      "t = 100, loss = 1.107504\n",
      "progress is as such: [0.999875, 0.8118, 0.0009401988906738085, 0.8948965051617378]\n",
      "Starting epoch 77 / 100\n",
      "t = 100, loss = 0.000902\n",
      "t = 200, loss = 0.000977\n",
      "t = 300, loss = 0.001128\n",
      "t = 400, loss = 0.000792\n",
      "t = 500, loss = 0.001052\n",
      "t = 600, loss = 0.000923\n",
      "train acc: 0.99985 val acc:0.8112\n",
      "t = 100, loss = 1.111755\n",
      "progress is as such: [0.99985, 0.8112, 0.0009396261750505521, 0.8880991553648924]\n",
      "Starting epoch 78 / 100\n",
      "t = 100, loss = 0.000896\n",
      "t = 200, loss = 0.000858\n",
      "t = 300, loss = 0.001120\n",
      "t = 400, loss = 0.000751\n",
      "t = 500, loss = 0.001091\n",
      "t = 600, loss = 0.000885\n",
      "train acc: 0.999875 val acc:0.8101\n",
      "t = 100, loss = 1.127224\n",
      "progress is as such: [0.999875, 0.8101, 0.0009171656476190457, 0.8896009971698126]\n",
      "Starting epoch 79 / 100\n",
      "t = 100, loss = 0.000901\n",
      "t = 200, loss = 0.000801\n",
      "t = 300, loss = 0.001080\n",
      "t = 400, loss = 0.000719\n",
      "t = 500, loss = 0.000995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 600, loss = 0.000856\n",
      "train acc: 0.99985 val acc:0.8094\n",
      "t = 100, loss = 1.153011\n",
      "progress is as such: [0.99985, 0.8094, 0.0008914829709399969, 0.9018440726093757]\n",
      "Starting epoch 80 / 100\n",
      "t = 100, loss = 0.000839\n",
      "t = 200, loss = 0.000679\n",
      "t = 300, loss = 0.000988\n",
      "t = 400, loss = 0.000701\n",
      "t = 500, loss = 0.000893\n",
      "t = 600, loss = 0.000841\n",
      "train acc: 0.999875 val acc:0.8094\n",
      "t = 100, loss = 1.180071\n",
      "progress is as such: [0.999875, 0.8094, 0.0008620031727239108, 0.926566637861423]\n",
      "Starting epoch 81 / 100\n",
      "t = 100, loss = 0.000768\n",
      "t = 200, loss = 0.000629\n",
      "t = 300, loss = 0.000934\n",
      "t = 400, loss = 0.000723\n",
      "t = 500, loss = 0.452335\n",
      "t = 600, loss = 0.238370\n",
      "train acc: 0.93145 val acc:0.7761\n",
      "t = 100, loss = 0.642366\n",
      "progress is as such: [0.93145, 0.7761, 0.12517193273211327, 0.8017828781635333]\n",
      "Starting epoch 82 / 100\n",
      "t = 100, loss = 0.061869\n",
      "t = 200, loss = 0.222979\n",
      "t = 300, loss = 0.226471\n",
      "t = 400, loss = 0.125682\n",
      "t = 500, loss = 0.090142\n",
      "t = 600, loss = 0.015279\n",
      "train acc: 0.99135 val acc:0.7955\n",
      "t = 100, loss = 0.969919\n",
      "progress is as such: [0.99135, 0.7955, 0.14262072012449303, 0.843784725245757]\n",
      "Starting epoch 83 / 100\n",
      "t = 100, loss = 0.054287\n",
      "t = 200, loss = 0.071210\n",
      "t = 300, loss = 0.021751\n",
      "t = 400, loss = 0.004651\n",
      "t = 500, loss = 0.005947\n",
      "t = 600, loss = 0.005665\n",
      "train acc: 0.996275 val acc:0.8004\n",
      "t = 100, loss = 1.051246\n",
      "progress is as such: [0.996275, 0.8004, 0.02182183654095309, 0.9686937204156166]\n",
      "Starting epoch 84 / 100\n",
      "t = 100, loss = 0.033355\n",
      "t = 200, loss = 0.006092\n",
      "t = 300, loss = 0.001949\n",
      "t = 400, loss = 0.006264\n",
      "t = 500, loss = 0.002021\n",
      "t = 600, loss = 0.012435\n",
      "train acc: 0.99215 val acc:0.7935\n",
      "t = 100, loss = 1.257622\n",
      "progress is as such: [0.99215, 0.7935, 0.013010152203675646, 1.0493913418971574]\n",
      "Starting epoch 85 / 100\n",
      "t = 100, loss = 0.003874\n",
      "t = 200, loss = 0.020959\n",
      "t = 300, loss = 0.077097\n",
      "t = 400, loss = 0.070927\n",
      "t = 500, loss = 0.010076\n",
      "t = 600, loss = 0.016421\n",
      "train acc: 0.979275 val acc:0.7892\n",
      "t = 100, loss = 1.288577\n",
      "progress is as such: [0.979275, 0.7892, 0.03712647824058644, 1.0555380563705394]\n",
      "Starting epoch 86 / 100\n",
      "t = 100, loss = 0.022664\n",
      "t = 200, loss = 0.010149\n",
      "t = 300, loss = 0.027524\n",
      "t = 400, loss = 0.004623\n",
      "t = 500, loss = 0.082883\n",
      "t = 600, loss = 0.031625\n",
      "train acc: 0.9932 val acc:0.797\n",
      "t = 100, loss = 1.160132\n",
      "progress is as such: [0.9932, 0.797, 0.032828690887142256, 1.0080438118714552]\n",
      "Starting epoch 87 / 100\n",
      "t = 100, loss = 0.071426\n",
      "t = 200, loss = 0.002686\n",
      "t = 300, loss = 0.006828\n",
      "t = 400, loss = 0.076889\n",
      "t = 500, loss = 0.002259\n",
      "t = 600, loss = 0.002298\n",
      "train acc: 0.9973 val acc:0.799\n",
      "t = 100, loss = 0.882797\n",
      "progress is as such: [0.9973, 0.799, 0.010312243215500927, 1.0255018457388267]\n",
      "Starting epoch 88 / 100\n",
      "t = 100, loss = 0.000732\n",
      "t = 200, loss = 0.004002\n",
      "t = 300, loss = 0.001206\n",
      "t = 400, loss = 0.001314\n",
      "t = 500, loss = 0.001990\n",
      "t = 600, loss = 0.005564\n",
      "train acc: 0.997575 val acc:0.8047\n",
      "t = 100, loss = 0.921468\n",
      "progress is as such: [0.997575, 0.8047, 0.006457946681154844, 1.0674910090672665]\n",
      "Starting epoch 89 / 100\n",
      "t = 100, loss = 0.004469\n",
      "t = 200, loss = 0.017952\n",
      "t = 300, loss = 0.108913\n",
      "t = 400, loss = 0.081020\n",
      "t = 500, loss = 0.014133\n",
      "t = 600, loss = 0.024290\n",
      "train acc: 0.984525 val acc:0.7907\n",
      "t = 100, loss = 0.926513\n",
      "progress is as such: [0.984525, 0.7907, 0.03182655205436719, 1.085325880883596]\n",
      "Starting epoch 90 / 100\n",
      "t = 100, loss = 0.051786\n",
      "t = 200, loss = 0.022468\n",
      "t = 300, loss = 0.062475\n",
      "t = 400, loss = 0.038072\n",
      "t = 500, loss = 0.005577\n",
      "t = 600, loss = 0.006762\n",
      "train acc: 0.994475 val acc:0.7948\n",
      "t = 100, loss = 1.091754\n",
      "progress is as such: [0.994475, 0.7948, 0.03227668950775973, 1.0286141501214259]\n",
      "Starting epoch 91 / 100\n",
      "t = 100, loss = 0.075731\n",
      "t = 200, loss = 0.005015\n",
      "t = 300, loss = 0.001190\n",
      "t = 400, loss = 0.008128\n",
      "t = 500, loss = 0.002070\n",
      "t = 600, loss = 0.001550\n",
      "train acc: 0.998375 val acc:0.8033\n",
      "t = 100, loss = 1.076547\n",
      "progress is as such: [0.998375, 0.8033, 0.008496539052337026, 1.038454505113455]\n",
      "Starting epoch 92 / 100\n",
      "t = 100, loss = 0.002469\n",
      "t = 200, loss = 0.001096\n",
      "t = 300, loss = 0.002127\n",
      "t = 400, loss = 0.000983\n",
      "t = 500, loss = 0.001500\n",
      "t = 600, loss = 0.000533\n",
      "train acc: 0.9993 val acc:0.8061\n",
      "t = 100, loss = 0.935604\n",
      "progress is as such: [0.9993, 0.8061, 0.0024880429085057517, 1.0085900521431215]\n",
      "Starting epoch 93 / 100\n",
      "t = 100, loss = 0.000625\n",
      "t = 200, loss = 0.000746\n",
      "t = 300, loss = 0.001164\n",
      "t = 400, loss = 0.000633\n",
      "t = 500, loss = 0.000791\n",
      "t = 600, loss = 0.000582\n",
      "train acc: 0.9996 val acc:0.8081\n",
      "t = 100, loss = 0.881365\n",
      "progress is as such: [0.9996, 0.8081, 0.0008061236510865199, 0.9692069769669802]\n",
      "Starting epoch 94 / 100\n",
      "t = 100, loss = 0.000744\n",
      "t = 200, loss = 0.000408\n",
      "t = 300, loss = 0.000948\n",
      "t = 400, loss = 0.000663\n",
      "t = 500, loss = 0.000863\n",
      "t = 600, loss = 0.000709\n",
      "train acc: 0.999675 val acc:0.8075\n",
      "t = 100, loss = 0.852129\n",
      "progress is as such: [0.999675, 0.8075, 0.000764299840785754, 0.9382806013409908]\n",
      "Starting epoch 95 / 100\n",
      "t = 100, loss = 0.000750\n",
      "t = 200, loss = 0.000503\n",
      "t = 300, loss = 0.000932\n",
      "t = 400, loss = 0.000698\n",
      "t = 500, loss = 0.000918\n",
      "t = 600, loss = 0.000795\n",
      "train acc: 0.999725 val acc:0.8083\n",
      "t = 100, loss = 0.838816\n",
      "progress is as such: [0.999725, 0.8083, 0.000826919188675208, 0.9198415115093573]\n",
      "Starting epoch 96 / 100\n",
      "t = 100, loss = 0.000731\n",
      "t = 200, loss = 0.000558\n",
      "t = 300, loss = 0.000893\n",
      "t = 400, loss = 0.000701\n",
      "t = 500, loss = 0.000921\n",
      "t = 600, loss = 0.000796\n",
      "train acc: 0.9998 val acc:0.8085\n",
      "t = 100, loss = 0.835104\n",
      "progress is as such: [0.9998, 0.8085, 0.0008373492134687228, 0.9091447775180523]\n",
      "Starting epoch 97 / 100\n",
      "t = 100, loss = 0.000716\n",
      "t = 200, loss = 0.000592\n",
      "t = 300, loss = 0.000845\n",
      "t = 400, loss = 0.000702\n",
      "t = 500, loss = 0.000932\n",
      "t = 600, loss = 0.000813\n",
      "train acc: 0.9998 val acc:0.8094\n",
      "t = 100, loss = 0.838341\n",
      "progress is as such: [0.9998, 0.8094, 0.0008343638828358589, 0.9030862387556297]\n",
      "Starting epoch 98 / 100\n",
      "t = 100, loss = 0.000705\n",
      "t = 200, loss = 0.000611\n",
      "t = 300, loss = 0.000801\n",
      "t = 400, loss = 0.000694\n",
      "t = 500, loss = 0.000890\n",
      "t = 600, loss = 0.000813\n",
      "train acc: 0.99985 val acc:0.8092\n",
      "t = 100, loss = 0.836113\n",
      "progress is as such: [0.99985, 0.8092, 0.0008213095581875398, 0.8998008903402549]\n",
      "Starting epoch 99 / 100\n",
      "t = 100, loss = 0.000699\n",
      "t = 200, loss = 0.000624\n",
      "t = 300, loss = 0.000775\n",
      "t = 400, loss = 0.000733\n",
      "t = 500, loss = 0.000892\n",
      "t = 600, loss = 0.000836\n",
      "train acc: 0.9999 val acc:0.8108\n",
      "t = 100, loss = 0.840372\n",
      "progress is as such: [0.9999, 0.8108, 0.0008065266152605033, 0.9027846940816977]\n",
      "Starting epoch 100 / 100\n",
      "t = 100, loss = 0.000695\n",
      "t = 200, loss = 0.000643\n",
      "t = 300, loss = 0.000789\n",
      "t = 400, loss = 0.000763\n",
      "t = 500, loss = 0.000882\n",
      "t = 600, loss = 0.000799\n",
      "train acc: 0.9999 val acc:0.8086\n",
      "t = 100, loss = 0.882403\n",
      "progress is as such: [0.9999, 0.8086, 0.000794254482174531, 0.9100342656557376]\n",
      "validation accuracy is 0.8133. Training time for 100 epochs: 2080.31 sec\n",
      "The best validation accuracy is 0.8133.\n"
     ]
    }
   ],
   "source": [
    "model = generator().type(gpu_dtype)\n",
    "epochs = 100\n",
    "best_val_acc=0.0\n",
    "learnRate = 0.001\n",
    "reg = 5e-04\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(gpu_dtype)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learnRate, weight_decay=reg)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "best_val_acc, results = train_detailed(model, loss_fn, optimizer, reg, num_epochs=epochs, verbose=True)\n",
    "\n",
    "if best_val_acc>val_acc:  \n",
    "    torch.save(model.state_dict(), 'model_bestacc40_3.pt')\n",
    "\n",
    "end = time.time()\n",
    "    \n",
    "print('validation accuracy is {0}. Training time for {1} epochs: {2:.2f} sec'.format(best_val_acc, epochs, end-start))\n",
    "print('The best validation accuracy is {0}.'.format(best_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001350851717672993\n"
     ]
    }
   ],
   "source": [
    "print (optimizer.param_groups[0][\"lr\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential (\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU (inplace)\n",
      "  (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU (inplace)\n",
      "  (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (7): ReLU (inplace)\n",
      "  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (10): ReLU (inplace)\n",
      "  (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (12): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (13): ReLU (inplace)\n",
      "  (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (16): ReLU (inplace)\n",
      "  (17): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (18): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (19): ReLU (inplace)\n",
      "  (20): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU (inplace)\n",
      "  (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (25): ReLU (inplace)\n",
      "  (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (28): ReLU (inplace)\n",
      "  (29): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (30): Flatten (\n",
      "  )\n",
      "  (31): Linear (2048 -> 1024)\n",
      "  (32): ReLU (inplace)\n",
      "  (33): Linear (1024 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.77025000e-01   5.59900000e-01   1.44091235e+00   1.22546016e+00]\n",
      " [  7.40075000e-01   7.06100000e-01   9.82078682e-01   8.46668047e-01]\n",
      " [  7.70525000e-01   7.21700000e-01   7.76951505e-01   8.18472776e-01]\n",
      " [  8.00600000e-01   7.41800000e-01   6.47296960e-01   8.04583665e-01]\n",
      " [  8.41825000e-01   7.58500000e-01   5.46922907e-01   7.69834310e-01]\n",
      " [  8.42100000e-01   7.40800000e-01   4.33770679e-01   9.05505705e-01]\n",
      " [  8.86500000e-01   7.66300000e-01   3.65865528e-01   8.20362541e-01]\n",
      " [  9.03825000e-01   7.69100000e-01   3.07028844e-01   8.72687706e-01]\n",
      " [  9.06000000e-01   7.65800000e-01   2.66192146e-01   8.53320745e-01]\n",
      " [  9.16950000e-01   7.64300000e-01   2.30957278e-01   8.90901817e-01]\n",
      " [  9.41150000e-01   7.75700000e-01   1.85002305e-01   9.18468232e-01]\n",
      " [  9.43400000e-01   7.72700000e-01   1.55292990e-01   9.51691782e-01]\n",
      " [  9.35225000e-01   7.62700000e-01   1.49106281e-01   9.62881412e-01]\n",
      " [  9.53850000e-01   7.80900000e-01   1.38364963e-01   8.98824468e-01]\n",
      " [  9.52350000e-01   7.78800000e-01   1.27114123e-01   9.59174768e-01]\n",
      " [  9.60175000e-01   7.78300000e-01   1.07932880e-01   9.63065272e-01]\n",
      " [  9.67800000e-01   7.86100000e-01   9.92982779e-02   9.40979383e-01]\n",
      " [  9.65325000e-01   7.87000000e-01   9.58331924e-02   9.61890566e-01]\n",
      " [  9.61950000e-01   7.81700000e-01   1.01025585e-01   9.67375942e-01]\n",
      " [  9.62950000e-01   7.81700000e-01   9.73710815e-02   9.50334222e-01]\n",
      " [  9.70425000e-01   7.82700000e-01   7.43710706e-02   9.97833465e-01]\n",
      " [  9.69650000e-01   7.84000000e-01   7.38828105e-02   9.93224184e-01]\n",
      " [  9.69925000e-01   7.84800000e-01   7.92062556e-02   9.85559756e-01]\n",
      " [  9.71175000e-01   7.94000000e-01   8.14553979e-02   9.83009690e-01]\n",
      " [  9.74125000e-01   7.85600000e-01   7.51588637e-02   1.00142336e+00]\n",
      " [  9.76050000e-01   7.85800000e-01   5.67856502e-02   1.04036110e+00]\n",
      " [  9.70200000e-01   7.83000000e-01   5.96939869e-02   1.03273242e+00]\n",
      " [  9.75250000e-01   7.87800000e-01   6.45748073e-02   1.01460788e+00]\n",
      " [  9.77225000e-01   7.88900000e-01   6.61058717e-02   9.78462685e-01]\n",
      " [  9.70225000e-01   7.84600000e-01   6.47265902e-02   1.01769043e+00]\n",
      " [  9.80650000e-01   7.88800000e-01   4.92326185e-02   1.01271189e+00]\n",
      " [  9.79700000e-01   7.82600000e-01   4.91931579e-02   1.05330885e+00]\n",
      " [  9.80125000e-01   7.92200000e-01   5.42123662e-02   9.94541782e-01]\n",
      " [  9.82325000e-01   7.93800000e-01   5.79629682e-02   9.93329988e-01]\n",
      " [  9.79075000e-01   7.89300000e-01   5.08278151e-02   1.03334652e+00]\n",
      " [  9.86050000e-01   7.90700000e-01   3.86142603e-02   1.03092138e+00]\n",
      " [  9.77875000e-01   7.83700000e-01   3.63570829e-02   1.06341851e+00]\n",
      " [  9.76875000e-01   7.80100000e-01   5.23858700e-02   1.07816820e+00]\n",
      " [  9.81525000e-01   7.81900000e-01   4.15165032e-02   1.07611785e+00]\n",
      " [  9.79950000e-01   7.86900000e-01   4.63879085e-02   1.05888351e+00]\n",
      " [  9.88625000e-01   7.89600000e-01   3.49973474e-02   1.07110594e+00]\n",
      " [  9.82250000e-01   7.86500000e-01   2.91086689e-02   1.09021033e+00]\n",
      " [  9.87950000e-01   7.90600000e-01   4.18473016e-02   1.05014649e+00]\n",
      " [  9.85450000e-01   7.87600000e-01   3.66701295e-02   1.06239857e+00]\n",
      " [  9.85375000e-01   7.91700000e-01   4.06359223e-02   1.04725168e+00]\n",
      " [  9.91275000e-01   7.99700000e-01   2.74448863e-02   1.01629981e+00]\n",
      " [  9.81775000e-01   7.84200000e-01   2.40340498e-02   1.12776483e+00]\n",
      " [  9.85575000e-01   7.87900000e-01   3.70064143e-02   1.05688577e+00]\n",
      " [  9.84075000e-01   7.87500000e-01   3.28351866e-02   1.08071949e+00]\n",
      " [  9.89425000e-01   7.93100000e-01   2.69046045e-02   1.06173224e+00]\n",
      " [  9.91025000e-01   7.98500000e-01   2.32784889e-02   1.03031702e+00]\n",
      " [  9.88500000e-01   7.93300000e-01   2.79822149e-02   1.03437314e+00]\n",
      " [  9.90750000e-01   7.95300000e-01   2.35747204e-02   1.05673498e+00]\n",
      " [  9.90400000e-01   7.90500000e-01   2.36976449e-02   1.08674832e+00]\n",
      " [  9.87825000e-01   7.86200000e-01   3.51170314e-02   1.05196156e+00]\n",
      " [  9.96175000e-01   7.98500000e-01   1.75242798e-02   1.03155341e+00]\n",
      " [  9.95300000e-01   7.99500000e-01   1.05344487e-02   1.06774939e+00]\n",
      " [  9.83425000e-01   7.87100000e-01   1.87850131e-02   1.12504164e+00]\n",
      " [  9.91525000e-01   7.94900000e-01   4.07627264e-02   1.00725662e+00]\n",
      " [  9.94975000e-01   7.98100000e-01   1.77343734e-02   1.03006609e+00]\n",
      " [  9.97300000e-01   8.02400000e-01   9.82995657e-03   1.03020780e+00]\n",
      " [  9.90525000e-01   7.91700000e-01   9.93750246e-03   1.11777551e+00]\n",
      " [  9.90050000e-01   7.93300000e-01   3.33685130e-02   1.06292592e+00]\n",
      " [  9.91975000e-01   7.95800000e-01   2.31125586e-02   1.06811355e+00]\n",
      " [  9.91025000e-01   7.96800000e-01   1.35504865e-02   1.09957086e+00]\n",
      " [  9.95550000e-01   8.00200000e-01   1.22048575e-02   1.06415413e+00]\n",
      " [  9.94300000e-01   7.98100000e-01   8.90999036e-03   1.09158464e+00]\n",
      " [  9.90375000e-01   7.94000000e-01   2.17134417e-02   1.10037101e+00]\n",
      " [  9.92225000e-01   7.94200000e-01   1.68290066e-02   1.12201703e+00]\n",
      " [  9.92325000e-01   7.95000000e-01   1.80382689e-02   1.08446436e+00]\n",
      " [  9.96575000e-01   7.94200000e-01   1.32487664e-02   1.08136039e+00]\n",
      " [  9.97675000e-01   8.03800000e-01   6.78780737e-03   1.05638131e+00]\n",
      " [  9.99550000e-01   8.09100000e-01   2.65086799e-03   1.00760856e+00]\n",
      " [  9.99750000e-01   8.13200000e-01   9.17962585e-04   9.50104215e-01]\n",
      " [  9.99850000e-01   8.13300000e-01   8.90069617e-04   9.11812998e-01]\n",
      " [  9.99875000e-01   8.11800000e-01   9.40198891e-04   8.94896505e-01]\n",
      " [  9.99850000e-01   8.11200000e-01   9.39626175e-04   8.88099155e-01]\n",
      " [  9.99875000e-01   8.10100000e-01   9.17165648e-04   8.89600997e-01]\n",
      " [  9.99850000e-01   8.09400000e-01   8.91482971e-04   9.01844073e-01]\n",
      " [  9.99875000e-01   8.09400000e-01   8.62003173e-04   9.26566638e-01]\n",
      " [  9.31450000e-01   7.76100000e-01   1.25171933e-01   8.01782878e-01]\n",
      " [  9.91350000e-01   7.95500000e-01   1.42620720e-01   8.43784725e-01]\n",
      " [  9.96275000e-01   8.00400000e-01   2.18218365e-02   9.68693720e-01]\n",
      " [  9.92150000e-01   7.93500000e-01   1.30101522e-02   1.04939134e+00]\n",
      " [  9.79275000e-01   7.89200000e-01   3.71264782e-02   1.05553806e+00]\n",
      " [  9.93200000e-01   7.97000000e-01   3.28286909e-02   1.00804381e+00]\n",
      " [  9.97300000e-01   7.99000000e-01   1.03122432e-02   1.02550185e+00]\n",
      " [  9.97575000e-01   8.04700000e-01   6.45794668e-03   1.06749101e+00]\n",
      " [  9.84525000e-01   7.90700000e-01   3.18265521e-02   1.08532588e+00]\n",
      " [  9.94475000e-01   7.94800000e-01   3.22766895e-02   1.02861415e+00]\n",
      " [  9.98375000e-01   8.03300000e-01   8.49653905e-03   1.03845451e+00]\n",
      " [  9.99300000e-01   8.06100000e-01   2.48804291e-03   1.00859005e+00]\n",
      " [  9.99600000e-01   8.08100000e-01   8.06123651e-04   9.69206977e-01]\n",
      " [  9.99675000e-01   8.07500000e-01   7.64299841e-04   9.38280601e-01]\n",
      " [  9.99725000e-01   8.08300000e-01   8.26919189e-04   9.19841512e-01]\n",
      " [  9.99800000e-01   8.08500000e-01   8.37349213e-04   9.09144778e-01]\n",
      " [  9.99800000e-01   8.09400000e-01   8.34363883e-04   9.03086239e-01]\n",
      " [  9.99850000e-01   8.09200000e-01   8.21309558e-04   8.99800890e-01]\n",
      " [  9.99900000e-01   8.10800000e-01   8.06526615e-04   9.02784694e-01]\n",
      " [  9.99900000e-01   8.08600000e-01   7.94254482e-04   9.10034266e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7967"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = model\n",
    "#best_model.load_state_dict(torch.load('model_bestacc40_2.pt'))\n",
    "\n",
    "check_accuracy(best_model, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAEYCAYAAABBfQDEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDvppmmVQE94AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fabb02ae400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(list(range(1,len(np.array(results)[:,2])+1)), np.array(results)[:,2], label='Training')\n",
    "plt.plot(list(range(1,len(np.array(results)[:,3])+1)), np.array(results)[:,3], label='Validation')\n",
    "plt.title('Cross Entropy Loss vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(list(range(1,len(np.array(results)[:,0])+1)), np.array(results)[:,0], label='Training')\n",
    "plt.plot(list(range(1,len(np.array(results)[:,1])+1)), np.array(results)[:,1], label='Validation')\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('sgd_model_plot_batch_norm_hidden_256x256.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
